{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6da721a-f83d-4c14-ac97-517d3ac8ea6f",
   "metadata": {},
   "source": [
    "# Build a Customer Support Bot\n",
    "\n",
    "Customer support bots can free up teams' time by handling routine issues, but it can be hard to build a bot that reliably handles diverse tasks in a way that doesn't leave the user pulling their hair out.\n",
    "\n",
    "In this tutorial, you will build a customer support bot for an airline to help users research and make travel arrangements. You'll learn to use LangGraph's interrupts and checkpointers and more complex state to organize your assistant's tools and manage a user's flight bookings, hotel reservations, car rentals, and excursions. It assumes you are familiar with the concepts presented in the [LangGraph introductory tutorial](https://langchain-ai.github.io/langgraph/tutorials/introduction/).\n",
    "\n",
    "By the end, you'll have built a working bot and gained an understanding of  LangGraph's key concepts and architectures. You'll be able to apply these design patterns to your other AI projects.\n",
    "\n",
    "Your final chat bot will look something like the following diagram:\n",
    "\n",
    "![Final Diagram](../img/part-4-diagram.png)\n",
    "\n",
    "Let's start!\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "First, set up your environment. We'll install this tutorial's prerequisites, download the test DB, and define the tools we will reuse in each section.\n",
    "\n",
    "We'll be using Claude as our LLM and define a number of custom tools. While most of our tools will connect to a local sqlite database (and require no additional dependencies), we will also provide a general web search to the agent using Tavily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afc570bf-e129-415b-8f2d-8bbce08131ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "! pip install -U langgraph langchain-community langchain-anthropic tavily-python pandas langchain-community langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98d31507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-community in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (0.0.38)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement langchain-ollama (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for langchain-ollama\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade langchain-community langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "358e5666-b7c5-4e46-90a1-7ea273d86ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "#_set_env(\"ANTHROPIC_API_KEY\")\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "\n",
    "# Recommended\n",
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Customer Support Bot Tutorial\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58121817-b31e-496d-9e46-2bec02c63300",
   "metadata": {},
   "source": [
    "#### Populate the database\n",
    "\n",
    "Run the next script to fetch a `sqlite` DB we've prepared for this tutorial and update it to look like it's current. The details are unimportant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71638c2a-5038-439e-907a-de2bb548db34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sqlite3\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "db_url = \"https://storage.googleapis.com/benchmarks-artifacts/travel-db/travel2.sqlite\"\n",
    "local_file = \"travel2.sqlite\"\n",
    "# The backup lets us restart for each tutorial section\n",
    "backup_file = \"travel2.backup.sqlite\"\n",
    "overwrite = False\n",
    "if overwrite or not os.path.exists(local_file):\n",
    "    response = requests.get(db_url)\n",
    "    response.raise_for_status()  # Ensure the request was successful\n",
    "    with open(local_file, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    # Backup - we will use this to \"reset\" our DB in each section\n",
    "    shutil.copy(local_file, backup_file)\n",
    "# Convert the flights to present time for our tutorial\n",
    "conn = sqlite3.connect(local_file)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "tables = pd.read_sql(\n",
    "    \"SELECT name FROM sqlite_master WHERE type='table';\", conn\n",
    ").name.tolist()\n",
    "tdf = {}\n",
    "for t in tables:\n",
    "    tdf[t] = pd.read_sql(f\"SELECT * from {t}\", conn)\n",
    "\n",
    "example_time = pd.to_datetime(\n",
    "    tdf[\"flights\"][\"actual_departure\"].replace(\"\\\\N\", pd.NaT)\n",
    ").max()\n",
    "current_time = pd.to_datetime(\"now\").tz_localize(example_time.tz)\n",
    "time_diff = current_time - example_time\n",
    "\n",
    "tdf[\"bookings\"][\"book_date\"] = (\n",
    "    pd.to_datetime(tdf[\"bookings\"][\"book_date\"].replace(\"\\\\N\", pd.NaT), utc=True)\n",
    "    + time_diff\n",
    ")\n",
    "\n",
    "datetime_columns = [\n",
    "    \"scheduled_departure\",\n",
    "    \"scheduled_arrival\",\n",
    "    \"actual_departure\",\n",
    "    \"actual_arrival\",\n",
    "]\n",
    "for column in datetime_columns:\n",
    "    tdf[\"flights\"][column] = (\n",
    "        pd.to_datetime(tdf[\"flights\"][column].replace(\"\\\\N\", pd.NaT)) + time_diff\n",
    "    )\n",
    "\n",
    "for table_name, df in tdf.items():\n",
    "    df.to_sql(table_name, conn, if_exists=\"replace\", index=False)\n",
    "del df\n",
    "del tdf\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "db = local_file  # We'll be using this local file as our DB in this tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3aa34e-923b-49a1-8f34-54a1b2a90825",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "Next, define our assistant's tools to search the airline's policy manual and search and manage reservations for flights, hotels, car rentals, and excursions. We will reuse these tools throughout the tutorial. The exact implementations\n",
    "aren't important, so feel free to run the code below and jump to [Part 1](#part-1-zero-shot).\n",
    "\n",
    "#### Lookup Company Policies\n",
    "\n",
    "The assistant retrieve policy information to answer user questions. Note that _enforcement_ of these policies still must be done within the tools/APIs themselves, since the LLM can always ignore this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c646542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (0.10.37)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.3.0,>=0.1.4 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index) (0.2.5)\n",
      "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index) (0.1.12)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.35 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index) (0.10.37)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index) (0.1.9)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index) (0.1.6)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index) (0.9.48)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.13 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index) (0.1.19)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index) (0.1.6)\n",
      "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index) (0.1.6)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index) (0.1.3)\n",
      "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index) (0.1.22)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse<0.2.0,>=0.1.2 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index) (0.1.4)\n",
      "Requirement already satisfied: openai>=1.14.0 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index-agent-openai<0.3.0,>=0.1.4->llama-index) (1.30.1)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.35->llama-index) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (0.6.6)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (2024.3.1)\n",
      "Requirement already satisfied: httpx in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (0.27.0)\n",
      "Requirement already satisfied: jsonpath-ng<2.0.0,>=1.6.0 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (1.6.1)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (0.1.19)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (3.3)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (3.8.1)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (1.26.4)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (2.2.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (10.3.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (2.31.0)\n",
      "Requirement already satisfied: spacy<4.0.0,>=3.7.1 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (3.7.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (8.3.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (4.11.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.35->llama-index) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.12.3)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.2.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse<0.5.0,>=0.4.0 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index) (0.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama-index) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama-index) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.9.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (2.5)\n",
      "Requirement already satisfied: ply in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from jsonpath-ng<2.0.0,>=1.6.0->llama-index-core<0.11.0,>=0.10.35->llama-index) (3.11)\n",
      "Requirement already satisfied: pydantic>=1.10 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.35->llama-index) (2.7.1)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama-index) (4.2.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama-index) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.0.5)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama-index) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.35->llama-index) (0.14.0)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (2024.5.10)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.3.0,>=0.1.4->llama-index) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.35->llama-index) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.35->llama-index) (2.1.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (6.4.0)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (3.4.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.35->llama-index) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.35->llama-index) (3.21.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.35->llama-index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.35->llama-index) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.35->llama-index) (2024.1)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.35->llama-index) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.35->llama-index) (2.18.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.16.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (0.1.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from jinja2->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.35->llama-index) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "#! pip install llama-index-embeddings-ollama\n",
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "563e8c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \n",
      "verifying sha256 digest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "removing any unused layers \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "654e2f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "#import openai\n",
    "#from langchain_community.vectorstores import VectorStoreRetriever\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "response = requests.get(\n",
    "    \"https://storage.googleapis.com/benchmarks-artifacts/travel-db/swiss_faq.md\"\n",
    ")\n",
    "response.raise_for_status()\n",
    "faq_text = response.text\n",
    "\n",
    "docs = [{\"page_content\": txt} for txt in re.split(r\"(?=\\n##)\", faq_text)]\n",
    "\n",
    "\n",
    "class VectorStoreRetriever:\n",
    "    def __init__(self, docs: list, vectors: list, oai_client):\n",
    "        self._arr = np.array(vectors)\n",
    "        self._docs = docs\n",
    "        self._client = oai_client\n",
    "\n",
    "    @classmethod\n",
    "    def from_docs(cls, docs, oai_client):\n",
    "        # embeddings = oai_client.embeddings.create(\n",
    "        #     model=\"text-embedding-3-small\", input=[doc[\"page_content\"] for doc in docs]\n",
    "        # )\n",
    "        # vectors = [emb.embedding for emb in embeddings.data]\n",
    "        embeddings = oai_client.embed_documents( [doc[\"page_content\"] for doc in docs])\n",
    "        vectors  = [embedding for embedding in embeddings]# [emb.embedding for emb in embeddings.data]\n",
    "       # print(embeddings)\n",
    "        return cls(docs, vectors, oai_client)\n",
    "\n",
    "    def query(self, query: str, k: int = 5) -> list[dict]:\n",
    "        # embed = self._client.embeddings.create(\n",
    "        #     model=\"text-embedding-3-small\", input=[query]\n",
    "        # )\n",
    "        # # \"@\" is just a matrix multiplication in python\n",
    "        # scores = np.array(embed.data[0].embedding) @ self._arr.T\n",
    "        embed =  self._client.embed_query( query)\n",
    "        embed_vector = np.array(embed[0])  # Assuming embed_query returns a list of vectors\n",
    "        scores = embed_vector @ self._arr.T\n",
    "        top_k_idx = np.argpartition(scores, -k)[-k:]\n",
    "        top_k_idx_sorted = top_k_idx[np.argsort(-scores[top_k_idx])]\n",
    "        return [\n",
    "            {**self._docs[idx], \"similarity\": scores[idx]} for idx in top_k_idx_sorted\n",
    "        ]\n",
    "\n",
    "\n",
    "retriever = VectorStoreRetriever.from_docs(docs, OllamaEmbeddings(model='nomic-embed-text')) #OllamaClient())\n",
    "\n",
    "\n",
    "@tool\n",
    "def lookup_policy(query: str) -> str:\n",
    "    \"\"\"Consult the company policies to check whether certain options are permitted.\n",
    "    Use this before making any flight changes performing other 'write' events.\"\"\"\n",
    "    docs = retriever.query(query, k=2)\n",
    "    return \"\\n\\n\".join([doc[\"page_content\"] for doc in docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3556949",
   "metadata": {},
   "source": [
    "#### Flights\n",
    "\n",
    "Define the (`fetch_user_flight_information`) tool to let the agent see the current user's flight information.  Then define tools to search for flights and manage the passenger's bookings stored in the SQL database.\n",
    "\n",
    "We use `ensure_config` to pass in the `passenger_id` in via configurable parameters. The LLM never has to provide these explicitly, they are provided for a given invocation of the graph so that each user cannot access other passengers' booking information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "043b4341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from datetime import date, datetime, timedelta\n",
    "from typing import Optional\n",
    "\n",
    "import pytz\n",
    "from langchain_core.runnables import ensure_config\n",
    "\n",
    "\n",
    "@tool\n",
    "def fetch_user_flight_information() -> list[dict]:\n",
    "    \"\"\"Fetch all tickets for the user along with corresponding flight information and seat assignments.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries where each dictionary contains the ticket details,\n",
    "        associated flight details, and the seat assignments for each ticket belonging to the user.\n",
    "    \"\"\"\n",
    "    config = ensure_config()  # Fetch from the context\n",
    "    configuration = config.get(\"configurable\", {})\n",
    "    passenger_id = configuration.get(\"passenger_id\", None)\n",
    "    if not passenger_id:\n",
    "        raise ValueError(\"No passenger ID configured.\")\n",
    "\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        t.ticket_no, t.book_ref,\n",
    "        f.flight_id, f.flight_no, f.departure_airport, f.arrival_airport, f.scheduled_departure, f.scheduled_arrival,\n",
    "        bp.seat_no, tf.fare_conditions\n",
    "    FROM \n",
    "        tickets t\n",
    "        JOIN ticket_flights tf ON t.ticket_no = tf.ticket_no\n",
    "        JOIN flights f ON tf.flight_id = f.flight_id\n",
    "        JOIN boarding_passes bp ON bp.ticket_no = t.ticket_no AND bp.flight_id = f.flight_id\n",
    "    WHERE \n",
    "        t.passenger_id = ?\n",
    "    \"\"\"\n",
    "    cursor.execute(query, (passenger_id,))\n",
    "    rows = cursor.fetchall()\n",
    "    column_names = [column[0] for column in cursor.description]\n",
    "    results = [dict(zip(column_names, row)) for row in rows]\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "@tool\n",
    "def search_flights(\n",
    "    departure_airport: Optional[str] = None,\n",
    "    arrival_airport: Optional[str] = None,\n",
    "    start_time: Optional[date | datetime] = None,\n",
    "    end_time: Optional[date | datetime] = None,\n",
    "    limit: int = 20,\n",
    ") -> list[dict]:\n",
    "    \"\"\"Search for flights based on departure airport, arrival airport, and departure time range.\"\"\"\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    query = \"SELECT * FROM flights WHERE 1 = 1\"\n",
    "    params = []\n",
    "\n",
    "    if departure_airport:\n",
    "        query += \" AND departure_airport = ?\"\n",
    "        params.append(departure_airport)\n",
    "\n",
    "    if arrival_airport:\n",
    "        query += \" AND arrival_airport = ?\"\n",
    "        params.append(arrival_airport)\n",
    "\n",
    "    if start_time:\n",
    "        query += \" AND scheduled_departure >= ?\"\n",
    "        params.append(start_time)\n",
    "\n",
    "    if end_time:\n",
    "        query += \" AND scheduled_departure <= ?\"\n",
    "        params.append(end_time)\n",
    "    query += \" LIMIT ?\"\n",
    "    params.append(limit)\n",
    "    cursor.execute(query, params)\n",
    "    rows = cursor.fetchall()\n",
    "    column_names = [column[0] for column in cursor.description]\n",
    "    results = [dict(zip(column_names, row)) for row in rows]\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "@tool\n",
    "def update_ticket_to_new_flight(ticket_no: str, new_flight_id: int) -> str:\n",
    "    \"\"\"Update the user's ticket to a new valid flight.\"\"\"\n",
    "    config = ensure_config()\n",
    "    configuration = config.get(\"configurable\", {})\n",
    "    passenger_id = configuration.get(\"passenger_id\", None)\n",
    "    if not passenger_id:\n",
    "        raise ValueError(\"No passenger ID configured.\")\n",
    "\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\n",
    "        \"SELECT departure_airport, arrival_airport, scheduled_departure FROM flights WHERE flight_id = ?\",\n",
    "        (new_flight_id,),\n",
    "    )\n",
    "    new_flight = cursor.fetchone()\n",
    "    if not new_flight:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return \"Invalid new flight ID provided.\"\n",
    "    column_names = [column[0] for column in cursor.description]\n",
    "    new_flight_dict = dict(zip(column_names, new_flight))\n",
    "    timezone = pytz.timezone(\"Etc/GMT-3\")\n",
    "    current_time = datetime.now(tz=timezone)\n",
    "    departure_time = datetime.strptime(\n",
    "        new_flight_dict[\"scheduled_departure\"], \"%Y-%m-%d %H:%M:%S.%f%z\"\n",
    "    )\n",
    "    time_until = (departure_time - current_time).total_seconds()\n",
    "    if time_until < (3 * 3600):\n",
    "        return f\"Not permitted to reschedule to a flight that is less than 3 hours from the current time. Selected flight is at {departure_time}.\"\n",
    "\n",
    "    cursor.execute(\n",
    "        \"SELECT flight_id FROM ticket_flights WHERE ticket_no = ?\", (ticket_no,)\n",
    "    )\n",
    "    current_flight = cursor.fetchone()\n",
    "    if not current_flight:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return \"No existing ticket found for the given ticket number.\"\n",
    "\n",
    "    # Check the signed-in user actually has this ticket\n",
    "    cursor.execute(\n",
    "        \"SELECT * FROM tickets WHERE ticket_no = ? AND passenger_id = ?\",\n",
    "        (ticket_no, passenger_id),\n",
    "    )\n",
    "    current_ticket = cursor.fetchone()\n",
    "    if not current_ticket:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return f\"Current signed-in passenger with ID {passenger_id} not the owner of ticket {ticket_no}\"\n",
    "\n",
    "    # In a real application, you'd likely add additional checks here to enforce business logic,\n",
    "    # like \"does the new departure airport match the current ticket\", etc.\n",
    "    # While it's best to try to be *proactive* in 'type-hinting' policies to the LLM\n",
    "    # it's inevitably going to get things wrong, so you **also** need to ensure your\n",
    "    # API enforces valid behavior\n",
    "    cursor.execute(\n",
    "        \"UPDATE ticket_flights SET flight_id = ? WHERE ticket_no = ?\",\n",
    "        (new_flight_id, ticket_no),\n",
    "    )\n",
    "    conn.commit()\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    return \"Ticket successfully updated to new flight.\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def cancel_ticket(ticket_no: str) -> str:\n",
    "    \"\"\"Cancel the user's ticket and remove it from the database.\"\"\"\n",
    "    config = ensure_config()\n",
    "    configuration = config.get(\"configurable\", {})\n",
    "    passenger_id = configuration.get(\"passenger_id\", None)\n",
    "    if not passenger_id:\n",
    "        raise ValueError(\"No passenger ID configured.\")\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\n",
    "        \"SELECT flight_id FROM ticket_flights WHERE ticket_no = ?\", (ticket_no,)\n",
    "    )\n",
    "    existing_ticket = cursor.fetchone()\n",
    "    if not existing_ticket:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return \"No existing ticket found for the given ticket number.\"\n",
    "\n",
    "    # Check the signed-in user actually has this ticket\n",
    "    cursor.execute(\n",
    "        \"SELECT flight_id FROM tickets WHERE ticket_no = ? AND passenger_id = ?\",\n",
    "        (ticket_no, passenger_id),\n",
    "    )\n",
    "    current_ticket = cursor.fetchone()\n",
    "    if not current_ticket:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return f\"Current signed-in passenger with ID {passenger_id} not the owner of ticket {ticket_no}\"\n",
    "\n",
    "    cursor.execute(\"DELETE FROM ticket_flights WHERE ticket_no = ?\", (ticket_no,))\n",
    "    conn.commit()\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    return \"Ticket successfully cancelled.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf77f8f-a051-46cd-be0b-7fe69121a3c1",
   "metadata": {},
   "source": [
    "#### Car Rental Tools\n",
    "\n",
    "Once a user books a flight, they likely will want to organize transportation. Define some \"car rental\" tools to let the user search for and reserve a car at their destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3edabaf-7a23-4f9f-9c57-97b799bc21df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime\n",
    "from typing import Optional, Union\n",
    "\n",
    "\n",
    "@tool\n",
    "def search_car_rentals(\n",
    "    location: Optional[str] = None,\n",
    "    name: Optional[str] = None,\n",
    "    price_tier: Optional[str] = None,\n",
    "    start_date: Optional[Union[datetime, date]] = None,\n",
    "    end_date: Optional[Union[datetime, date]] = None,\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Search for car rentals based on location, name, price tier, start date, and end date.\n",
    "\n",
    "    Args:\n",
    "        location (Optional[str]): The location of the car rental. Defaults to None.\n",
    "        name (Optional[str]): The name of the car rental company. Defaults to None.\n",
    "        price_tier (Optional[str]): The price tier of the car rental. Defaults to None.\n",
    "        start_date (Optional[Union[datetime, date]]): The start date of the car rental. Defaults to None.\n",
    "        end_date (Optional[Union[datetime, date]]): The end date of the car rental. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of car rental dictionaries matching the search criteria.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    query = \"SELECT * FROM car_rentals WHERE 1=1\"\n",
    "    params = []\n",
    "\n",
    "    if location:\n",
    "        query += \" AND location LIKE ?\"\n",
    "        params.append(f\"%{location}%\")\n",
    "    if name:\n",
    "        query += \" AND name LIKE ?\"\n",
    "        params.append(f\"%{name}%\")\n",
    "    # For our tutorial, we will let you match on any dates and price tier.\n",
    "    # (since our toy dataset doesn't have much data)\n",
    "    cursor.execute(query, params)\n",
    "    results = cursor.fetchall()\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    return [\n",
    "        dict(zip([column[0] for column in cursor.description], row)) for row in results\n",
    "    ]\n",
    "\n",
    "\n",
    "@tool\n",
    "def book_car_rental(rental_id: int) -> str:\n",
    "    \"\"\"\n",
    "    Book a car rental by its ID.\n",
    "\n",
    "    Args:\n",
    "        rental_id (int): The ID of the car rental to book.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating whether the car rental was successfully booked or not.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\"UPDATE car_rentals SET booked = 1 WHERE id = ?\", (rental_id,))\n",
    "    conn.commit()\n",
    "\n",
    "    if cursor.rowcount > 0:\n",
    "        conn.close()\n",
    "        return f\"Car rental {rental_id} successfully booked.\"\n",
    "    else:\n",
    "        conn.close()\n",
    "        return f\"No car rental found with ID {rental_id}.\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def update_car_rental(\n",
    "    rental_id: int,\n",
    "    start_date: Optional[Union[datetime, date]] = None,\n",
    "    end_date: Optional[Union[datetime, date]] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Update a car rental's start and end dates by its ID.\n",
    "\n",
    "    Args:\n",
    "        rental_id (int): The ID of the car rental to update.\n",
    "        start_date (Optional[Union[datetime, date]]): The new start date of the car rental. Defaults to None.\n",
    "        end_date (Optional[Union[datetime, date]]): The new end date of the car rental. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating whether the car rental was successfully updated or not.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    if start_date:\n",
    "        cursor.execute(\n",
    "            \"UPDATE car_rentals SET start_date = ? WHERE id = ?\",\n",
    "            (start_date, rental_id),\n",
    "        )\n",
    "    if end_date:\n",
    "        cursor.execute(\n",
    "            \"UPDATE car_rentals SET end_date = ? WHERE id = ?\", (end_date, rental_id)\n",
    "        )\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "    if cursor.rowcount > 0:\n",
    "        conn.close()\n",
    "        return f\"Car rental {rental_id} successfully updated.\"\n",
    "    else:\n",
    "        conn.close()\n",
    "        return f\"No car rental found with ID {rental_id}.\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def cancel_car_rental(rental_id: int) -> str:\n",
    "    \"\"\"\n",
    "    Cancel a car rental by its ID.\n",
    "\n",
    "    Args:\n",
    "        rental_id (int): The ID of the car rental to cancel.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating whether the car rental was successfully cancelled or not.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\"UPDATE car_rentals SET booked = 0 WHERE id = ?\", (rental_id,))\n",
    "    conn.commit()\n",
    "\n",
    "    if cursor.rowcount > 0:\n",
    "        conn.close()\n",
    "        return f\"Car rental {rental_id} successfully cancelled.\"\n",
    "    else:\n",
    "        conn.close()\n",
    "        return f\"No car rental found with ID {rental_id}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86c5712-d2b1-492a-a7b7-4396aa4ec339",
   "metadata": {},
   "source": [
    "#### Hotels\n",
    "\n",
    "The user has to sleep! Define some tools to search for and manage hotel reservations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8e4ab3c-0086-4257-855b-97cc4037513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def search_hotels(\n",
    "    location: Optional[str] = None,\n",
    "    name: Optional[str] = None,\n",
    "    price_tier: Optional[str] = None,\n",
    "    checkin_date: Optional[Union[datetime, date]] = None,\n",
    "    checkout_date: Optional[Union[datetime, date]] = None,\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Search for hotels based on location, name, price tier, check-in date, and check-out date.\n",
    "\n",
    "    Args:\n",
    "        location (Optional[str]): The location of the hotel. Defaults to None.\n",
    "        name (Optional[str]): The name of the hotel. Defaults to None.\n",
    "        price_tier (Optional[str]): The price tier of the hotel. Defaults to None. Examples: Midscale, Upper Midscale, Upscale, Luxury\n",
    "        checkin_date (Optional[Union[datetime, date]]): The check-in date of the hotel. Defaults to None.\n",
    "        checkout_date (Optional[Union[datetime, date]]): The check-out date of the hotel. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of hotel dictionaries matching the search criteria.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    query = \"SELECT * FROM hotels WHERE 1=1\"\n",
    "    params = []\n",
    "\n",
    "    if location:\n",
    "        query += \" AND location LIKE ?\"\n",
    "        params.append(f\"%{location}%\")\n",
    "    if name:\n",
    "        query += \" AND name LIKE ?\"\n",
    "        params.append(f\"%{name}%\")\n",
    "    # For the sake of this tutorial, we will let you match on any dates and price tier.\n",
    "    cursor.execute(query, params)\n",
    "    results = cursor.fetchall()\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    return [\n",
    "        dict(zip([column[0] for column in cursor.description], row)) for row in results\n",
    "    ]\n",
    "\n",
    "\n",
    "@tool\n",
    "def book_hotel(hotel_id: int) -> str:\n",
    "    \"\"\"\n",
    "    Book a hotel by its ID.\n",
    "\n",
    "    Args:\n",
    "        hotel_id (int): The ID of the hotel to book.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating whether the hotel was successfully booked or not.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\"UPDATE hotels SET booked = 1 WHERE id = ?\", (hotel_id,))\n",
    "    conn.commit()\n",
    "\n",
    "    if cursor.rowcount > 0:\n",
    "        conn.close()\n",
    "        return f\"Hotel {hotel_id} successfully booked.\"\n",
    "    else:\n",
    "        conn.close()\n",
    "        return f\"No hotel found with ID {hotel_id}.\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def update_hotel(\n",
    "    hotel_id: int,\n",
    "    checkin_date: Optional[Union[datetime, date]] = None,\n",
    "    checkout_date: Optional[Union[datetime, date]] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Update a hotel's check-in and check-out dates by its ID.\n",
    "\n",
    "    Args:\n",
    "        hotel_id (int): The ID of the hotel to update.\n",
    "        checkin_date (Optional[Union[datetime, date]]): The new check-in date of the hotel. Defaults to None.\n",
    "        checkout_date (Optional[Union[datetime, date]]): The new check-out date of the hotel. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating whether the hotel was successfully updated or not.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    if checkin_date:\n",
    "        cursor.execute(\n",
    "            \"UPDATE hotels SET checkin_date = ? WHERE id = ?\", (checkin_date, hotel_id)\n",
    "        )\n",
    "    if checkout_date:\n",
    "        cursor.execute(\n",
    "            \"UPDATE hotels SET checkout_date = ? WHERE id = ?\",\n",
    "            (checkout_date, hotel_id),\n",
    "        )\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "    if cursor.rowcount > 0:\n",
    "        conn.close()\n",
    "        return f\"Hotel {hotel_id} successfully updated.\"\n",
    "    else:\n",
    "        conn.close()\n",
    "        return f\"No hotel found with ID {hotel_id}.\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def cancel_hotel(hotel_id: int) -> str:\n",
    "    \"\"\"\n",
    "    Cancel a hotel by its ID.\n",
    "\n",
    "    Args:\n",
    "        hotel_id (int): The ID of the hotel to cancel.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating whether the hotel was successfully cancelled or not.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\"UPDATE hotels SET booked = 0 WHERE id = ?\", (hotel_id,))\n",
    "    conn.commit()\n",
    "\n",
    "    if cursor.rowcount > 0:\n",
    "        conn.close()\n",
    "        return f\"Hotel {hotel_id} successfully cancelled.\"\n",
    "    else:\n",
    "        conn.close()\n",
    "        return f\"No hotel found with ID {hotel_id}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f08190c-21f6-4a07-b9e2-3aa991fe4eed",
   "metadata": {},
   "source": [
    "#### Excursions\n",
    "\n",
    "Finally, define some tools to let the user search for things to do (and make reservations) once they arrive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2260eccb-8ae2-4a41-a1ba-f78ee3df3010",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def search_trip_recommendations(\n",
    "    location: Optional[str] = None,\n",
    "    name: Optional[str] = None,\n",
    "    keywords: Optional[str] = None,\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Search for trip recommendations based on location, name, and keywords.\n",
    "\n",
    "    Args:\n",
    "        location (Optional[str]): The location of the trip recommendation. Defaults to None.\n",
    "        name (Optional[str]): The name of the trip recommendation. Defaults to None.\n",
    "        keywords (Optional[str]): The keywords associated with the trip recommendation. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of trip recommendation dictionaries matching the search criteria.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    query = \"SELECT * FROM trip_recommendations WHERE 1=1\"\n",
    "    params = []\n",
    "\n",
    "    if location:\n",
    "        query += \" AND location LIKE ?\"\n",
    "        params.append(f\"%{location}%\")\n",
    "    if name:\n",
    "        query += \" AND name LIKE ?\"\n",
    "        params.append(f\"%{name}%\")\n",
    "    if keywords:\n",
    "        keyword_list = keywords.split(\",\")\n",
    "        keyword_conditions = \" OR \".join([\"keywords LIKE ?\" for _ in keyword_list])\n",
    "        query += f\" AND ({keyword_conditions})\"\n",
    "        params.extend([f\"%{keyword.strip()}%\" for keyword in keyword_list])\n",
    "\n",
    "    cursor.execute(query, params)\n",
    "    results = cursor.fetchall()\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    return [\n",
    "        dict(zip([column[0] for column in cursor.description], row)) for row in results\n",
    "    ]\n",
    "\n",
    "\n",
    "@tool\n",
    "def book_excursion(recommendation_id: int) -> str:\n",
    "    \"\"\"\n",
    "    Book a excursion by its recommendation ID.\n",
    "\n",
    "    Args:\n",
    "        recommendation_id (int): The ID of the trip recommendation to book.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating whether the trip recommendation was successfully booked or not.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\n",
    "        \"UPDATE trip_recommendations SET booked = 1 WHERE id = ?\", (recommendation_id,)\n",
    "    )\n",
    "    conn.commit()\n",
    "\n",
    "    if cursor.rowcount > 0:\n",
    "        conn.close()\n",
    "        return f\"Trip recommendation {recommendation_id} successfully booked.\"\n",
    "    else:\n",
    "        conn.close()\n",
    "        return f\"No trip recommendation found with ID {recommendation_id}.\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def update_excursion(recommendation_id: int, details: str) -> str:\n",
    "    \"\"\"\n",
    "    Update a trip recommendation's details by its ID.\n",
    "\n",
    "    Args:\n",
    "        recommendation_id (int): The ID of the trip recommendation to update.\n",
    "        details (str): The new details of the trip recommendation.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating whether the trip recommendation was successfully updated or not.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\n",
    "        \"UPDATE trip_recommendations SET details = ? WHERE id = ?\",\n",
    "        (details, recommendation_id),\n",
    "    )\n",
    "    conn.commit()\n",
    "\n",
    "    if cursor.rowcount > 0:\n",
    "        conn.close()\n",
    "        return f\"Trip recommendation {recommendation_id} successfully updated.\"\n",
    "    else:\n",
    "        conn.close()\n",
    "        return f\"No trip recommendation found with ID {recommendation_id}.\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def cancel_excursion(recommendation_id: int) -> str:\n",
    "    \"\"\"\n",
    "    Cancel a trip recommendation by its ID.\n",
    "\n",
    "    Args:\n",
    "        recommendation_id (int): The ID of the trip recommendation to cancel.\n",
    "\n",
    "    Returns:\n",
    "        str: A message indicating whether the trip recommendation was successfully cancelled or not.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\n",
    "        \"UPDATE trip_recommendations SET booked = 0 WHERE id = ?\", (recommendation_id,)\n",
    "    )\n",
    "    conn.commit()\n",
    "\n",
    "    if cursor.rowcount > 0:\n",
    "        conn.close()\n",
    "        return f\"Trip recommendation {recommendation_id} successfully cancelled.\"\n",
    "    else:\n",
    "        conn.close()\n",
    "        return f\"No trip recommendation found with ID {recommendation_id}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf5d064",
   "metadata": {},
   "source": [
    "#### Utilities\n",
    "\n",
    "Define helper functions to pretty print the messages in the graph while we debug it and to give our tool node error handling (by adding the error to the chat history)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "663f001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "def handle_tool_error(state) -> dict:\n",
    "    error = state.get(\"error\")\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=f\"Error: {repr(error)}\\n please fix your mistakes.\",\n",
    "                tool_call_id=tc[\"id\"],\n",
    "            )\n",
    "            for tc in tool_calls\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def create_tool_node_with_fallback(tools: list) -> dict:\n",
    "    return ToolNode(tools).with_fallbacks(\n",
    "        [RunnableLambda(handle_tool_error)], exception_key=\"error\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _print_event(event: dict, _printed: set, max_length=1500):\n",
    "    current_state = event.get(\"dialog_state\")\n",
    "    if current_state:\n",
    "        print(f\"Currently in: \", current_state[-1])\n",
    "    message = event.get(\"messages\")\n",
    "    if message:\n",
    "        if isinstance(message, list):\n",
    "            message = message[-1]\n",
    "        if message.id not in _printed:\n",
    "            msg_repr = message.pretty_repr(html=True)\n",
    "            if len(msg_repr) > max_length:\n",
    "                msg_repr = msg_repr[:max_length] + \" ... (truncated)\"\n",
    "            print(msg_repr)\n",
    "            _printed.add(message.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa068b1a",
   "metadata": {},
   "source": [
    "## Part 1: Zero-shot Agent\n",
    "\n",
    "When building, it's best to start with the simplest working implementation and use an [evaluation tool like LangSmith](https://docs.smith.langchain.com/evaluation) to measure its efficacy. All else equal, prefer simple, scalable solutions to complicated ones. In this case, the single-graph approach has limitations. The bot may take undesired actions without user confirmation, struggle with complex queries, and lack focus in its responses. We'll address these issues later. \n",
    "\n",
    "In this section, we will define a simple Zero-shot agent as the assistant, give the agent **all** of our tools, and prompt it to use them judiciously to assist the user.\n",
    "\n",
    "The simple 2-node graph will look like the following:\n",
    "\n",
    "![Part 1 Diagram](../img/part-1-diagram.png)\n",
    "\n",
    "Start by defining the state.\n",
    "\n",
    "#### State\n",
    "\n",
    "Define our `StateGraph`'s state as a typed dictionary containing an append-only list of messages. These messages form the chat history, which is all the state our simple assistant needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3216948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897fbd63",
   "metadata": {},
   "source": [
    "#### Agent\n",
    "\n",
    "Next, define the assistant function. This function takes the graph state, formats it into a prompt, and then calls an LLM for it to predict the best response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "993c33e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import getpass\n",
    "# import os\n",
    "\n",
    "# os.environ[\"TAVILY_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cab75ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_experimental in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (0.0.58)\n",
      "Requirement already satisfied: langchain<0.2.0,>=0.1.17 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from langchain_experimental) (0.1.20)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.52 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from langchain_experimental) (0.1.52)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.17->langchain_experimental) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.17->langchain_experimental) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.17->langchain_experimental) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.17->langchain_experimental) (0.6.6)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.38 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.17->langchain_experimental) (0.0.38)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.17->langchain_experimental) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.17->langchain_experimental) (0.1.57)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.17->langchain_experimental) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.17->langchain_experimental) (2.7.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.17->langchain_experimental) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.17->langchain_experimental) (8.3.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain_experimental) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain_experimental) (23.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.17->langchain_experimental) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.17->langchain_experimental) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.17->langchain_experimental) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.17->langchain_experimental) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.17->langchain_experimental) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.17->langchain_experimental) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.17->langchain_experimental) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain_experimental) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain<0.2.0,>=0.1.17->langchain_experimental) (3.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.17->langchain_experimental) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.17->langchain_experimental) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.17->langchain_experimental) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.17->langchain_experimental) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.17->langchain_experimental) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.17->langchain_experimental) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.17->langchain_experimental) (2024.2.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/envs/langraph/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.17->langchain_experimental) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Solved by doing\n",
    "!pip install langchain_experimental\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fd269bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_anthropic import ChatAnthropic\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable, RunnableConfig\n",
    "\n",
    "\n",
    "class Assistant:\n",
    "    def __init__(self, runnable: Runnable):\n",
    "        self.runnable = runnable\n",
    "\n",
    "    def __call__(self, state: State, config: RunnableConfig):\n",
    "        while True:\n",
    "            passenger_id = config.get(\"passenger_id\", None)\n",
    "            state = {**state, \"user_info\": passenger_id}\n",
    "            result = self.runnable.invoke(state)\n",
    "            # If the LLM happens to return an empty response, we will re-prompt it\n",
    "            # for an actual response.\n",
    "            if not result.tool_calls and (\n",
    "                not result.content\n",
    "                or isinstance(result.content, list)\n",
    "                and not result.content[0].get(\"text\")\n",
    "            ):\n",
    "                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n",
    "                state = {**state, \"messages\": messages}\n",
    "            else:\n",
    "                break\n",
    "        return {\"messages\": result}\n",
    "\n",
    "\n",
    "# Haiku is faster and cheaper, but less accurate\n",
    "# llm = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "#from langchain_community.llms import Ollama\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "llm = OllamaFunctions(model=\"llama3\")\n",
    "#llm = Ollama(model=\"llama3\")\n",
    "\n",
    "#llm = ChatAnthropic(model=\"claude-3-sonnet-20240229\", temperature=1)\n",
    "# You could swap LLMs, though you will likely want to update the prompts when\n",
    "# doing so!\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")\n",
    "\n",
    "primary_assistant_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful customer support assistant for Swiss Airlines. \"\n",
    "            \" Use the provided tools to search for flights, company policies, and other information to assist the user's queries. \"\n",
    "            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n",
    "            \" If a search comes up empty, expand your search before giving up.\"\n",
    "            \"\\n\\nCurrent user:\\n<User>\\n{user_info}\\n</User>\",\n",
    "           # \"\\nCurrent time: {time}.\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ").partial(time=datetime.now())\n",
    "\n",
    "part_1_tools = [\n",
    "    TavilySearchResults(max_results=1),\n",
    "    fetch_user_flight_information,\n",
    "    search_flights,\n",
    "    lookup_policy,\n",
    "    update_ticket_to_new_flight,\n",
    "    cancel_ticket,\n",
    "    search_car_rentals,\n",
    "    book_car_rental,\n",
    "    update_car_rental,\n",
    "    cancel_car_rental,\n",
    "    search_hotels,\n",
    "    book_hotel,\n",
    "    update_hotel,\n",
    "    cancel_hotel,\n",
    "    search_trip_recommendations,\n",
    "    book_excursion,\n",
    "    update_excursion,\n",
    "    cancel_excursion,\n",
    "]\n",
    "part_1_assistant_runnable = primary_assistant_prompt | llm.bind_tools(part_1_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be1b8f1",
   "metadata": {},
   "source": [
    "#### Define Graph\n",
    "\n",
    "Now, create the graph. The graph is the final assistant for this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "36064ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", Assistant(part_1_assistant_runnable))\n",
    "builder.add_node(\"action\", create_tool_node_with_fallback(part_1_tools))\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.set_entry_point(\"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    tools_condition,\n",
    "    # \"action\" calls one of our tools. END causes the graph to terminate (and respond to the user)\n",
    "    {\"action\": \"action\", END: END},\n",
    ")\n",
    "builder.add_edge(\"action\", \"assistant\")\n",
    "\n",
    "# The checkpointer lets the graph persist its state\n",
    "# this is a complete memory for the entire graph.\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "part_1_graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4a7e47a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(part_1_graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1042d045-76c5-45f5-ae12-8f29d3184338",
   "metadata": {},
   "source": [
    "#### Example Conversation\n",
    "\n",
    "Now it's time to try out our mighty chatbot! Let's run it over the following list of dialog turns. If it hits a \"RecursionLimit\", that means the agent wasn't able to get an answer in the allocated number of steps. That's OK! We have more tricks up our sleeve in later sections of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b7443751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received event: {'messages': [HumanMessage(content='Hi there, what time is my flight?', id='cd12df36-f4c3-44e2-92af-6eb9578a38c5')]}\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi there, what time is my flight?\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type TavilySearchResults is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 44\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m tutorial_questions:\n\u001b[1;32m     40\u001b[0m     events \u001b[38;5;241m=\u001b[39m part_1_graph\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m     41\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, question)}, config, stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m     )\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m events:\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;66;03m#print(event)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;66;03m#_print_event(event, _printed)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived event:\u001b[39m\u001b[38;5;124m\"\u001b[39m, event)  \u001b[38;5;66;03m# Add this line for debugging\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langgraph/pregel/__init__.py:845\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m    838\u001b[0m done, inflight \u001b[38;5;241m=\u001b[39m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mwait(\n\u001b[1;32m    839\u001b[0m     futures,\n\u001b[1;32m    840\u001b[0m     return_when\u001b[38;5;241m=\u001b[39mconcurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFIRST_EXCEPTION,\n\u001b[1;32m    841\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m    842\u001b[0m )\n\u001b[1;32m    844\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m--> 845\u001b[0m _panic_or_proceed(done, inflight, step)\n\u001b[1;32m    847\u001b[0m \u001b[38;5;66;03m# combine pending writes from all tasks\u001b[39;00m\n\u001b[1;32m    848\u001b[0m pending_writes \u001b[38;5;241m=\u001b[39m deque[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1370\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(done, inflight, step)\u001b[0m\n\u001b[1;32m   1368\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m   1369\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m-> 1370\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   1371\u001b[0m         \u001b[38;5;66;03m# TODO this is where retry of an entire step would happen\u001b[39;00m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langchain_core/runnables/base.py:2499\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2498\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2499\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m   2500\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2501\u001b[0m             \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[1;32m   2502\u001b[0m             patch_config(\n\u001b[1;32m   2503\u001b[0m                 config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2504\u001b[0m             ),\n\u001b[1;32m   2505\u001b[0m         )\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langgraph/utils.py:88\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     82\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, config)\n\u001b[1;32m     83\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     84\u001b[0m         {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m: config}\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m accepts_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs\n\u001b[1;32m     87\u001b[0m     )\n\u001b[0;32m---> 88\u001b[0m     ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[57], line 15\u001b[0m, in \u001b[0;36mAssistant.__call__\u001b[0;34m(self, state, config)\u001b[0m\n\u001b[1;32m     13\u001b[0m passenger_id \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassenger_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m state \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mstate, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_info\u001b[39m\u001b[38;5;124m\"\u001b[39m: passenger_id}\n\u001b[0;32m---> 15\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunnable\u001b[38;5;241m.\u001b[39minvoke(state)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# If the LLM happens to return an empty response, we will re-prompt it\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# for an actual response.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39mtool_calls \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39mcontent[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langchain_core/runnables/base.py:2499\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2498\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2499\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m   2500\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2501\u001b[0m             \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[1;32m   2502\u001b[0m             patch_config(\n\u001b[1;32m   2503\u001b[0m                 config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2504\u001b[0m             ),\n\u001b[1;32m   2505\u001b[0m         )\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langchain_core/runnables/base.py:4525\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4520\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4522\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4523\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4524\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m   4526\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   4527\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[1;32m   4528\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   4529\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:158\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    154\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    155\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    157\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 158\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[1;32m    159\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    160\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    161\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    162\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    163\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    164\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    165\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    166\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    167\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    168\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:560\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    554\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    558\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    559\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:421\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    420\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 421\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    422\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    423\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    425\u001b[0m ]\n\u001b[1;32m    426\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:411\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 411\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[1;32m    412\u001b[0m                 m,\n\u001b[1;32m    413\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    414\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    415\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    416\u001b[0m             )\n\u001b[1;32m    417\u001b[0m         )\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:632\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 632\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m    633\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    634\u001b[0m         )\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    636\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langchain_experimental/llms/ollama_functions.py:304\u001b[0m, in \u001b[0;36mOllamaFunctions._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m     functions \u001b[38;5;241m=\u001b[39m [convert_to_ollama_tool(fn) \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m functions]\n\u001b[1;32m    300\u001b[0m system_message_prompt_template \u001b[38;5;241m=\u001b[39m SystemMessagePromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtool_system_prompt_template\n\u001b[1;32m    302\u001b[0m )\n\u001b[1;32m    303\u001b[0m system_message \u001b[38;5;241m=\u001b[39m system_message_prompt_template\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m--> 304\u001b[0m     tools\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mdumps(functions, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    305\u001b[0m )\n\u001b[1;32m    306\u001b[0m response_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m    307\u001b[0m     [system_message] \u001b[38;5;241m+\u001b[39m messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    308\u001b[0m )\n\u001b[1;32m    309\u001b[0m chat_generation_content \u001b[38;5;241m=\u001b[39m response_message\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/json/__init__.py:238\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    235\u001b[0m     skipkeys\u001b[38;5;241m=\u001b[39mskipkeys, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii,\n\u001b[1;32m    236\u001b[0m     check_circular\u001b[38;5;241m=\u001b[39mcheck_circular, allow_nan\u001b[38;5;241m=\u001b[39mallow_nan, indent\u001b[38;5;241m=\u001b[39mindent,\n\u001b[1;32m    237\u001b[0m     separators\u001b[38;5;241m=\u001b[39mseparators, default\u001b[38;5;241m=\u001b[39mdefault, sort_keys\u001b[38;5;241m=\u001b[39msort_keys,\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\u001b[38;5;241m.\u001b[39mencode(obj)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/json/encoder.py:202\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    200\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterencode(o, _one_shot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 202\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(chunks)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/json/encoder.py:430\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m _floatstr(o)\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 430\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/json/encoder.py:326\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_list\u001b[0;34m(lst, _current_indent_level)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 326\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    328\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/json/encoder.py:439\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCircular reference detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    438\u001b[0m     markers[markerid] \u001b[38;5;241m=\u001b[39m o\n\u001b[0;32m--> 439\u001b[0m o \u001b[38;5;241m=\u001b[39m _default(o)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/json/encoder.py:180\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    181\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type TavilySearchResults is not JSON serializable"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import uuid\n",
    "import json\n",
    "\n",
    "# Let's create an example conversation a user might have with the assistant\n",
    "tutorial_questions = [\n",
    "    \"Hi there, what time is my flight?\",\n",
    "    \"Am i allowed to update my flight to something sooner? I want to leave later today.\",\n",
    "    \"Update my flight to sometime next week then\",\n",
    "    \"The next available option is great\",\n",
    "    \"what about lodging and transportation?\",\n",
    "    \"Yeah i think i'd like an affordable hotel for my week-long stay (7 days). And I'll want to rent a car.\",\n",
    "    \"OK could you place a reservation for your recommended hotel? It sounds nice.\",\n",
    "    \"yes go ahead and book anything that's moderate expense and has availability.\",\n",
    "    \"Now for a car, what are my options?\",\n",
    "    \"Awesome let's just get the cheapest option. Go ahead and book for 7 days\",\n",
    "    \"Cool so now what recommendations do you have on excursions?\",\n",
    "    \"Are they available while I'm there?\",\n",
    "    \"interesting - i like the museums, what options are there? \",\n",
    "    \"OK great pick one and book it for my second day there.\",\n",
    "]\n",
    "\n",
    "# Update with the backup file so we can restart from the original place in each section\n",
    "shutil.copy(backup_file, db)\n",
    "thread_id = str(uuid.uuid4())\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # The passenger_id is used in our flight tools to\n",
    "        # fetch the user's flight information\n",
    "        \"passenger_id\": \"3442 587242\",\n",
    "        # Checkpoints are accessed by thread_id\n",
    "        \"thread_id\": thread_id,\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "_printed = set()\n",
    "for question in tutorial_questions:\n",
    "    events = part_1_graph.stream(\n",
    "        {\"messages\": (\"user\", question)}, config, stream_mode=\"values\"\n",
    "    )\n",
    "    \n",
    "    for event in events:\n",
    "        #print(event)\n",
    "        #_print_event(event, _printed)\n",
    "        try:\n",
    "            print(\"Received event:\", event)  # Add this line for debugging\n",
    "            _print_event(event, _printed)\n",
    "        except TypeError as e:\n",
    "            print(f\"Error processing event: {e}\")\n",
    "            # Attempt to log more details about the problematic object\n",
    "            try:\n",
    "                problematic_obj = json.dumps(event, default=str)\n",
    "                print(f\"Problematic event: {problematic_obj}\")\n",
    "            except Exception as inner_e:\n",
    "                print(f\"Error serializing event: {inner_e}\")\n",
    "                print(f\"Event data: {event}\")\n",
    "            raise  # Re-raise the error after logging for debugging purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aaae68-7791-4f5d-a98b-c0f3f9ed0eb0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Part 1 Review\n",
    "\n",
    "Our simple assistant is not bad! It was able to respond reasonably well for all the questions, quickly respond in-context, and successfully execute all our tasks. You can (check out an example LangSmith trace)[https://smith.langchain.com/public/f9e77b80-80ec-4837-98a8-254415cb49a1/r/26146720-d3f9-44b6-9bb9-9158cde61f9d] to get a better sense of how the LLM is prompted throughout the interactions above.\n",
    "\n",
    "If this were a simple Q&A bot, we'd probably be happy with the results above. Since our customer support bot is taking actions on behalf of the user, some of its behavior above is a bit concerning:\n",
    "\n",
    "1. The assistant booked a car when we were focusing on lodging, then had to cancel and rebook later on: oops! The user should have final say before booking to avoid unwanted feeds.\n",
    "2. The assistant struggled to search for recommendations. We could improve this by adding more verbose instructions and examples using the tool, but doing this for every tool can lead to a large prompt and overwhelmed agent.\n",
    "3. The assistant had to do an explicit search just to get the user's relevant information. We can save a lot of time by fetching the user's relevant travel details immediately so the assistant can directly respond.\n",
    "\n",
    "In the next section, we will address the first two of these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27abd8f6-80b5-47f4-809d-46372bd99e14",
   "metadata": {},
   "source": [
    "## Part 2: Add Confirmation\n",
    "\n",
    "When an assistant takes actions on behalf of the user, the user should (almost) always have the final say on whether to follow through with the actions. Otherwise, any small mistake the assistant makes (or any prompt injection it succombs to) can cause real damage to the user.\n",
    "\n",
    "In this section, we will use `interrupt_before` to pause the graph and return control to the user **before** executing any of the tools.\n",
    "\n",
    "Your graph will look something like the following:\n",
    "\n",
    "![Part 2 diagram](../img/part-2-diagram.png)\n",
    "\n",
    "As before, start by defining the state:\n",
    "\n",
    "#### State & Assistant\n",
    "\n",
    "Our graph state and LLM calling is nearly identical to Part 1 except:\n",
    "\n",
    "- We've added a `user_info` field that will be eagerly populated by our graph\n",
    "- We can use the state directly in the `Assistant` object rather than using the configurable params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c5098273-e1f6-46bf-b63b-172bbd3d9104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "#from langchain_anthropic import ChatAnthropic\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable, RunnableConfig\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    user_info: str\n",
    "\n",
    "\n",
    "class Assistant:\n",
    "    def __init__(self, runnable: Runnable):\n",
    "        self.runnable = runnable\n",
    "\n",
    "    def __call__(self, state: State, config: RunnableConfig):\n",
    "        while True:\n",
    "            passenger_id = config.get(\"passenger_id\", None)\n",
    "            result = self.runnable.invoke(state)\n",
    "            # If the LLM happens to return an empty response, we will re-prompt it\n",
    "            # for an actual response.\n",
    "            if not result.tool_calls and (\n",
    "                not result.content\n",
    "                or isinstance(result.content, list)\n",
    "                and not result.content[0].get(\"text\")\n",
    "            ):\n",
    "                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n",
    "                state = {**state, \"messages\": messages}\n",
    "            else:\n",
    "                break\n",
    "        return {\"messages\": result}\n",
    "\n",
    "\n",
    "# Haiku is faster and cheaper, but less accurate\n",
    "# llm = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "#llm = ChatAnthropic(model=\"claude-3-sonnet-20240229\", temperature=1)\n",
    "\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "llm = OllamaFunctions(model=\"llama3\")\n",
    "\n",
    "# You could also use OpenAI or another model, though you will likely have\n",
    "# to adapt the prompts\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")\n",
    "\n",
    "assistant_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful customer support assistant for Swiss Airlines. \"\n",
    "            \" Use the provided tools to search for flights, company policies, and other information to assist the user's queries. \"\n",
    "            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n",
    "            \" If a search comes up empty, expand your search before giving up.\"\n",
    "            \"\\n\\nCurrent user:\\n<User>\\n{user_info}\\n</User>\",\n",
    "            #\"\\nCurrent time: {time}.\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ").partial(time=datetime.now())\n",
    "\n",
    "part_2_tools = [\n",
    "    TavilySearchResults(max_results=1),\n",
    "    fetch_user_flight_information,\n",
    "    search_flights,\n",
    "    lookup_policy,\n",
    "    update_ticket_to_new_flight,\n",
    "    cancel_ticket,\n",
    "    search_car_rentals,\n",
    "    book_car_rental,\n",
    "    update_car_rental,\n",
    "    cancel_car_rental,\n",
    "    search_hotels,\n",
    "    book_hotel,\n",
    "    update_hotel,\n",
    "    cancel_hotel,\n",
    "    search_trip_recommendations,\n",
    "    book_excursion,\n",
    "    update_excursion,\n",
    "    cancel_excursion,\n",
    "]\n",
    "part_2_assistant_runnable = assistant_prompt | llm.bind_tools(part_2_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49733138-06b5-4260-830d-7182047d6bb8",
   "metadata": {},
   "source": [
    "#### Define Graph\n",
    "\n",
    "Now, create the graph. Make 2 changes from part 1 to address our previous concerns.\n",
    "\n",
    "1. Add an interrupt before using a tool\n",
    "2. Explicitly populate the user state within the first node so the assitant doesn't have to use a tool just to learn about the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "910002ce-2431-4280-854a-a273c517611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "\n",
    "def user_info(state: State):\n",
    "    return {\"user_info\": fetch_user_flight_information.invoke({})}\n",
    "\n",
    "\n",
    "# NEW: The fetch_user_info node runs first, meaning our assistant can see the user's flight information without\n",
    "# having to take an action\n",
    "builder.add_node(\"fetch_user_info\", user_info)\n",
    "builder.set_entry_point(\"fetch_user_info\")\n",
    "builder.add_node(\"assistant\", Assistant(part_2_assistant_runnable))\n",
    "builder.add_node(\"action\", create_tool_node_with_fallback(part_2_tools))\n",
    "builder.add_edge(\"fetch_user_info\", \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\", tools_condition, {\"action\": \"action\", END: END}\n",
    ")\n",
    "builder.add_edge(\"action\", \"assistant\")\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "part_2_graph = builder.compile(\n",
    "    checkpointer=memory,\n",
    "    # NEW: The graph will always halt before executing the \"action\" node.\n",
    "    # The user can approve or reject (or even alter the request) before\n",
    "    # the assistant continues\n",
    "    interrupt_before=[\"action\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "67f897be-3f83-4150-a235-8bc40f6c7117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/4gHYSUNDX1BST0ZJTEUAAQEAAAHIAAAAAAQwAABtbnRyUkdCIFhZWiAH4AABAAEAAAAAAABhY3NwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAA9tYAAQAAAADTLQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAlkZXNjAAAA8AAAACRyWFlaAAABFAAAABRnWFlaAAABKAAAABRiWFlaAAABPAAAABR3dHB0AAABUAAAABRyVFJDAAABZAAAAChnVFJDAAABZAAAAChiVFJDAAABZAAAAChjcHJ0AAABjAAAADxtbHVjAAAAAAAAAAEAAAAMZW5VUwAAAAgAAAAcAHMAUgBHAEJYWVogAAAAAAAAb6IAADj1AAADkFhZWiAAAAAAAABimQAAt4UAABjaWFlaIAAAAAAAACSgAAAPhAAAts9YWVogAAAAAAAA9tYAAQAAAADTLXBhcmEAAAAAAAQAAAACZmYAAPKnAAANWQAAE9AAAApbAAAAAAAAAABtbHVjAAAAAAAAAAEAAAAMZW5VUwAAACAAAAAcAEcAbwBvAGcAbABlACAASQBuAGMALgAgADIAMAAxADb/2wBDAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHBwYIDAoMDAsKCwsNDhIQDQ4RDgsLEBYQERMUFRUVDA8XGBYUGBIUFRT/2wBDAQMEBAUEBQkFBQkUDQsNFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBT/wAARCAEuANEDASIAAhEBAxEB/8QAHQABAAIDAQEBAQAAAAAAAAAAAAYHBAUIAwkBAv/EAFMQAAEDBAADAQoJBwgFDQAAAAECAwQABQYRBxIhExQVFiIxQVFVlOEIVFZhcXWSk9EXIzI4QnKzMzU3UoGRobEkYoKy1AkmQ0RFRnSDhJWW0uL/xAAbAQEAAgMBAQAAAAAAAAAAAAAAAwQBAgUGB//EADcRAAIBAgEKAwcEAQUAAAAAAAABAgMRBBITFBUhMUFRUpFhobEFInHB0eHwMjNigTQjQmOy8f/aAAwDAQACEQMRAD8A+qdKUoBSlKAUpSgMKberfbXA3LnRorhHMEPPJQSPTon5q8PCqy+uIHtKPxqv8ut8WfxNmd0xmZHLZ4nL2rYVr89K8m68/B61+rYf3CPwqnicbRwtTNSi27J8OKT+Z1KWCzsFPK3lieFVl9cQPaUfjTwqsvriB7Sj8arvwetfq2H9wj8KeD1r9Ww/uEfhVXWuH6Jd0S6u/l5FieFVl9cQPaUfjTwqsvriB7Sj8arvwetfq2H9wj8KeD1r9Ww/uEfhTWuH6Jd0NXfy8ixPCqy+uIHtKPxp4VWX1xA9pR+NV34PWv1bD+4R+FPB61+rYf3CPwprXD9Eu6Grv5eRYnhVZfXED2lH408KrL64ge0o/Gq78HrX6th/cI/Cng9a/VsP7hH4U1rh+iXdDV38vIsiPkVqlPIZYucN51Z0ltuQhSlH5gDWwqm5Npgw73jbkeFHYc76sjnbaSk60rzgVcldOlVhXpKrBNJ33+Bz8RRzEsm9xSlK3KwpSlAKUpQClKUApSlAKUpQClKUApSlAVnkf9J0/wCp4f8AGlV6V55H/SdP+p4f8aVXpXl/a3+W/hH/AKo9PhP2Yio7m3EGwcO7fHmX+f3E1JeEdhDbLj7rzhBPKhttKlqOgT0B0BUiqsOPNut0yy2Z+XAyZ2bDndtAuWJxVSJdue7NY7UoSDtBBKCkpUDzdR5xy6aUpJMszbUW0Yl5+Ebj9szfFLK0zNmQL9b3rgi4R7fLdKQlSEtpCEMknmKlcxOuTlHMBzipDfONuFY1lQx253ruO6dq0wpLkV7sUOOAFtK3wjs0lQUnQUoeUVU8S7ZpbrpwozjL8au0+YzarnAujdot5ekMuOrZLC3GG9lPOhnagOiVHR0KjHHS2ZdmEXiLbZtrzS53Lt2zj0C0NuotJhIS05zrKSEOO8wd2hwqVzBIQnyVcVGDkl8/GxVdWaTfy8DoSdxjxK35e/izlyedv7DrLT0GNBkPrbLoSW1KKGyAghadrJ5RvRINang3xvt3F9m6dzQ5kGRCmSWOzfhSEIU028W0L7RxpCedQAJbB5kbII6GsXhrapTfGLineHbdKixLmbSqLJkxltB9KYelBJUBvlUSCP2TsHRrC4Dvzsbm5RiV0sd3hS0Xy53Fqe7CX3DIYeklxstv/oFRS4PF3scqtgaqFwgouy27OPhtJFKTkr7tv2LhpSlVSya25fzvjf1qz/kqraqpbl/O+N/WrP8Akqrar2Xs/wDxIfFnn8f+6vgKUpV45gpSlAKUpQClKUApSlAKUpQClKUApSlAVnkf9J0/6nh/xpVRzKOGGIZtObm5BjFpvctpsMofnw23lpQCSEgqBIG1E6+c1ZGQcP4GQ3fvm7LnxJRYRGUYcjswpCVLUnY0fIVq/vrA/JVB9cXv233Vz8VgdJrZ6NTJ2JbnwSXyOxRxdOFNQkrlXngFw0KAg4FjhQCSE97GdAnWz+j8w/urf4pgGM4KJQxywW2xCVyl8W+KhnteXfLzcoG9cytb9JqY/kqg+uL37b7qfkqg+uL37b7qqP2XNqzrepKsbQW1RNbStl+SqD64vftvuqos6izbB8I3hfhkS93QWTIId0fnIXI24pTDSVN8qteL1J36a01P/wAq7M31hS5MsutZkONWnLbW5bb3bYt2t7hSpcWayl1tRB2CUqBHQ9akv5KoPri9+2+6n5KoPri9+2+6sr2Q1tVVdmY0+k9jTKtPwfuGR/7gY3/7Wz/9a2Fh4PYLi92Yulnw+yWu4sc3ZS4kBpp1vaSk8qgkEbBI+gmrC/JVB9cXv233U/JVB9cXv233Vu/Zc3sdb1NdNoL/AG+SI3cv53xv61Z/yVVtVDofC62xbhDmKn3SUuI6H225ErmRzjeiRrr5amNdWhRWHoxpZV7X8znYqtGtNSiKUpUpTFKUoBSlKAUpSgFKUoBSlKAUpSgFKUoBSlKAUpSgFc78Vf1zeBf1bff4CK6Irnfir+ubwL+rb7/ARQHRFKUoBSlKAUpSgFKUoBSlKAUpSgFKUoBSlKAUpSgFKUoBSlKAUpSgFKUoBXO/FX9c3gX9W33+AiuiK534q/rm8C/q2+/wEUB0RSlKAUpSgFKUoBSlKAUpSgFKUoBSlKAUpSgFKUoBSleMuWxAiuyZLzceOygrcedUEoQkDZJJ6AAec1lK+xA9qVApnEmXLWRZLOX2eoEu4umMhXzpRyqWR9IT6RvpvCOZ5aT0i2UD0FbxqbNNfqaX9lqOGqyV1EsqlVr4Z5d8Wsn2nqeGeXfFrJ9p6maXUu5nRK3IsqlVr4Z5d8Wsn2nqeGeXfFrJ9p6maXUu40StyJtlOOQswxm72G5IU5brpDegyUJVylTTqChYB83RR618J+KHCq88L+Kd5wWYyuTdIE3uVoNIJMlKtFpaEjZ8dKkKA8vjAeWvtF4Z5d8Wsn2nqqPNeDBzvjdinE64xLT35sDfKmOkudjJWklTK3Om+ZtSioEeUhO+idUzS6l3GiVuRZ3wXeC7PAbgtYcX5Ei5lHdl0cTo9pMcALnUeUJ0lsHzpbTVsVWvhnl3xayfaep4Z5d8Wsn2nqZpdS7jRK3IsqlVr4Z5d8Wsn2nqeGeXfFrJ9p6maXUu40StyLKpVa+GeXfFrJ9p6gzPLt9Y1l1+89TNLqXcaJW5FlUqvY3ES9xFg3KxMSGP2nbXKK3B/wCW4lO/7FE/NUzs18hZBBTLgP8AbsklJ2koUhQ8qVJUApKh50qAI9Faypyir714bSGdKdP9SM+lKVERClKUApSlAKUpQClKUAqs8kuqsov8iNvdptbobDYPiyJI0VLUPOGzoJB/bCjolKCLMqmcRWp6xNOr/lXnXnXf31OrUvfz8xNTR92nKa37F3v9PM6GCgpVLvgbSRIaiR3H33UMsNJK1uOKCUoSBskk9AAPPWJY8gteT25E+zXKHdoCyUplQX0PNKIOiApJIJB6Gqq+F1DmS+A9+ES5vW5KXI4fDTbaxIaU8htTSudJ0k84V00Tygb0SDoc4yrLcby6x8OMZdu8l2LZe+0652a3W5Ut4F4tIAbfU0w2naVFRSknqkAJ6mqh15VMmVmjoFbiW+XmUE8x5Rs62fRX7XLeWtZ7lLfCUZLOn4re28rkREOtRoZceb7lfLMpTf55tDnICkoCijalHX6Op6q5ZnnfEjJcXs+XKxmBikeEy/Mbt7EiTcJLzPalaw4nkQgJ5eiEgklWiAAKWCq34flrluMXeDJuUq3szY7s+KhDkiKh1KnWUr3yKWgHaQrlVokdeU68lZdc4TbPlknjRxMXYstFimwrBaHXpCbc0/3U6lEop2F7CEEhWwkb8YaUNdbn4W5Y9nnDbF8jktIYk3S2x5jzbf6KVrbClBO/NsnXzUNozynZolFKr6RlV0b+EBb8bTK1ZXcYk3FcXs0dZCJbDaV82ubolahreuvk3qqhm8T8/kY3Buq73NtuOsX29xLvfLZaWZr8NpiSpEUKZ5D+ZCUqC1hCleKNkbJpYxKqonT9K5yyDilmOVZpIsGIS7vMt9otUGU9dscgW+Qqc7JbUtLihLeQlDRSkEBsEklXjJ0N5ULKeJ+S5bgmOXG5eBVwuFhnzLw0xEjvuJcZkNNtuN83aJSpQUDy7WkBahokAhYxnVwTOg6xRdoKroq2CZHNySyJCoYdT2waKikOFG98pII3rWwRXtFaWxGabceVIcQgJU8sAKWQOqiAAAT5egArnninluQYVn3FCXBuiC7DwNN2t61wIxchuB15AQHOz51o5muflcKhtaulDec8hXZ0VWst2UWa73SdbYN3gTbjBIEuHHkocejk+TtEAkp8h8oFVLbrzmdtz3H8fuOWruDOW2GbKafbt8dpVrlNBkhbA5TzI0/0S72h2kbJGwa74V3S/wCC8AMRXarsl2+ZlkCbY1PlwmSIBcffLjukJSXlabWodoT4ywN66UsRurttb82fU6xrDcuC8Vm9/I+w22Eie0DpLzAPjKI860Dakny9CnyGq+4eXnI7XxFyPC8hvfhKmJb4l1h3JyK3Hf5HlvNracS0EoOlMbBCR0V13qrLdbQ80ttxIUhYKVJPkIPlFSU55uV+HHxRs0qsGmizkLS4kKSQpKhsEHYIr+qjXDN9yVw6xd11RW4u2RiVn9r82nxv7fL/AG1JalqQzc3Dk7HmGrOwpSlRmBSlKAUpSgFKUoBVUvQFY7kNwtbgKWX3XJ0JRPRba1cziR+4tZGvMlTfp1VrVq8hx2JksHuaVztqQrtGZDJCXWHACAtBIOjokaIIIJBBBIMkGrOEtz/LlmhVzM8rgVLn+EQeI2JTseuTshiFMLZcciqSlwcjiXBoqSoeVA83k3WpzvhNbc5vFuvQuV1x6/29tbDN2skhLL/YrIKml8yVJWgkA6Uk6I2NVNJlkySyrKXLcL2wN8sm3LShevNzNOKGj5vFUr09PIMIz7gDo45egf8AwoP+SqaPUf6dvwaO2qtGavdESv8Awft2R4lZrJKvN77otEpM6HehMCrg2+nn/OdopJCiQ4tJBSU6OtdBrAuvAqBcLozdouT5LZbyYTUGbcbZNQ27cW2wQhUgFspUsbVpaUpUNnR1U874T/k5evZPfTvhP+Tl69k99NHq8jOVRfFGgtPDK22i9326ty578q82+LbZJkPBf5uOlxKFAlOys9qoqKidnXQdd6a02rK+HNlteMYtjtsvFhtMRmJFmXW/rjyXEoQB46EQ1JB6eUHr5dDyVOO+E/5OXr2T3074T/k5evZPfTR6vIZdLhJIhE/h3NzyZbL/AHpyThmT29D0Vt/GrmJHPGcKCpta3Y6QQVIB1ybSUghVa9j4O9st+Lx7BbMryu0wW3pjriolwRzyBJXzuJdKm1c4B2EqI5xs+Nskmx++E/5OXr2T31pbhxCh2rJ7Tjsu3XRi93VDrkGEuL+ckJaAU4U9f2QQTTR6vIxlUXvaIzN+D3YEvWqRYrne8Ql2+3NWkSLFLS2t+K2NNtu9ohYXy9dK1zDZ61JIfDe3w8osd/7suEifaLU5aGTJfDnatLU0pS3FKHMpzbKfG5uu1bB303nfCf8AJy9eye+nfCf8nL17J76aPV5GVOitzRFpd94itynkxsOx96Mlag045kjqFLTvoSkQjykjzbOvSaw79whgZ6q8XG+qlW+5X3HU4/PjwJSXGmWudxZLS1NAlYU6ocyhogDxR1qa98J/ycvXsnvp3wn/ACcvXsnvpo9XkMum98r9jTv8Pbc/lGNX5T0oTLBDkQoqAtPZrQ8GgsrHLskdinWiB1PQ9NaVvgfjzfDKDhBenqt8B0SYk0PhEyO+Hi8h1DiUgJWlajogeToQdncy74T/AJOXr2T30E+eSB4OXof+k99NHq8hl0eaI9gfDCBgc263BNyud9vN0LYl3S8PpdfcQ2CG2xyJShKU8ytBKR1USd1IbwH5TCbdCJ74Tz3OzynqjfRTv0ISSo/RrykV7xYmRXVQREx96GD/ANYujrbTaf8AZQpSz9Gh9Iqa4tiDWPlcqQ8J92eTyOzC3yAJ8vI2nZ5Eb662ST5SdCto0808qdtnDf35Ir1cTTpxyYO7Nzb4LNsgRocdPIxHaSy2n0JSAAP7hWRSlRNtu7OEKUpWAKUpQClKUApSlAKUpQClKUApSlAKUpQCud+Kv65vAv6tvv8AARXRFc78Vf1zeBf1bff4CKA6IpSlAKUpQClKUApSlAKUpQClKUApSlAKUpQClKUApSlAKUpQClKUApSlAK534q/rm8C/q2+/wEV0RXO/FX9c3gX9W33+AigOiKUpQClKUApSlAKUpQClKUApSlAKUpQClKUApSlAKUpQClKUApSvxSglJJIAHUk+agP2la1zJbQ0spXdYSFDyhUhAP8AnX8+FVl9cQPaUfjUmbnyZmzGU3ObZcZu9wtttVebjEhvPxrcl3szKdSgqQ0F6PLzEBO9HW96NfL/ACz/AJRQ5LxowbPTw97lOLx58Y243rm7p7pbCN9p3OOTl1vXKrfzV9QPCqy+uIHtKPxr5jfCE+CazevhgWy3WGRHZxDMJHfJ+ZHcSWoAB3MSVbKUnyrQDoHtUJFM3PpYsz6BfB44tXPjhwvgZlccY8FEXBxwxISpplKcYSeUOlRbb5eZQXoaPQA7PN0suo9ZLjjGOWaBabbPtsS3QWG4saO3JQEtNISEoSOvkAAFZvhVZfXED2lH40zc+lizNpStWMosyjoXeAT6BJR+NbFl5uQ2HGlpcbV5FIOwf7a1cZR3owf3SlK1ApSlAKUpQClKUApSlAKUpQClKUApSlAKUpQGkyrJ28bhtlLJlz5Ci3FiBXL2itbJUrR5UJHVStHQ6AKUUpNeTbWu/O9vfpBuzuwoMLHLFaI8yGt6/tVzK+esqfKN4zS9y1kKTCWm3xx18VIQlbh+krVo+kIT6OnndbnFslsl3Gc8I8KIyt995QJCG0gqUo69ABNTTm6Puw2Pi+O3h+fQ7uFoRjBTlvZ4oxy0tpCU2uGlI8gEdAA/wr98H7X6th/cJ/ConYeOWD5JBvEuHfkIZtDHdU7u2O9FWwzontCh1CVFHinxgCDXk3x6wZ3Gnr935cbtrUhuIVPQJLbq3nBttCGlNhxZUOo5UnY61BnKnUy7lQ5omPg/a/VsP7hP4U8H7X6th/cJ/CoNO4x2ybdsGj2W5RuxyKW62nu6BMSp5ptDnOhtQbCW3QtH6LpT0SvpvVejXHzDblCvj1puLt2XaYr0txMeFIKHUNHlX2S+z5XQFEJJb5tbpnKnUzGXDmTXwftfq2H9wn8KeD9r9Ww/uE/hUBwvj5j+ScLImaXEybRF7COqW27Bk6aedSkhtrbQL45lhIW2FBXmrZR+OWDScSkZML+03ZY0tEGTIfZdaVHfWpKUodbUkLb6rT1UkAA7Oh1pnJ9TCnB7bks8H7X6th/cI/CvFnGodveMi1BVjmHr3RbdNEn/AFk65F/QtKh81Qu7cfMYYwjLsgtbz9ydxyIZEm3rhyI7+ykloFC2udKFkfynKUgbVvSSRIuHGeQ+I+Jwr1Dbksh1CA63JiPRyhwoSpQSHUJK0jm6LAKT5ietbKtVjukxeE/d3ll4jlrtyeVa7mlLdzbRzodbTytykA6KkjZ0odOZPm2CNg1KqpzI5ZtEJF6RpLtpcE0K6/oJ/lR0/rNlaf7auOpJWlFVFx9V/wCnCxVFUp+7uYpSlRFMUpSgFKUoBSlKAUpSgFKUoBSlKAUpSgKmTHMDJ8liLBCu7u6U7HRSHW0qBH+1zj6Umo/xWkZHE4b5G9iLfa5KiE4YKeUKPaa/ZSroVa2QD0J0DVmZtjD851m8WxoO3OMgtLj8wT3UzvZQCSAFg7KCrpsqSSkKKkxWDco9xSssr2ts8rrSwUuNK/qrQeqVfMQDUtZOf+ot2y/g/uegw1RVKeTfajlm1Y2xOzu6zZ+M8QMgxefhsq2y139h9cqY+Hm3FNIQtQLRKebl0G0FW+TqK2EFu43nDrxByW155c8Xtd0hOYzc+9qm7/EcS2sqdLYSFrS0rSQtSCVBwghQ610/SqtyXM+JzxbW83yiFwfn5JbJ70+Hk0px99yF2TwiCPKQy/JbRtLKlJLex0AUoDoTqvDhxAvMfI7rjOL2jJbVgci1zS7b8nglhu2zFK/NoiOnqttfMslIK0p0CCN6ro6lDbNbtpzVar9lkf4PuL2C3WLKrFcbF3std+LFsWmYIqUlt9cIkEOnxB4zfMQlex11UVViN3fZzxMDHcsMK43/ABufBXe2X35Mhlp9pDzilLKlDl7NRKVkKSjlJCR5Ov6Vm5q6N7XZRufYbeMg4j8SWoNvfLN24fpt0eSWylh2UXJgDfaHxeYBaNjewFA+Q1OuDOQLvfD+0Mv2e7WSVb4rEN+Pd4S4y+0Q0kK5QoeOnexzDYOqnFeUuYxb4zkiU83HjtjmW66oJSkekk9BRJt2RIoZLvc1eYsrmYzPhN7L05vuFoAbPO8ezT/isGrmSkJSEjoANCoBiOPvXm5Rr1NYUxCjErgsOgpccWQUl5aT+iACQkHr1JOulWBVqXuQVN71dv8Au2zyOLjKqqTSjwFKUqEoClKUApSlAKUpQClKUApSlAKUpQClKxLtdoVhtsm43KYxb4EVsuvypLgbaaQOpUpR0AB6TQGXVXcashxrE3Mdcu2OXe93O8XFq2RFWGM4qQgnaipbjZSUoQkLWQT1CVaB6627+VZFc+IzWPxMYcXhz1qMp3LG7ghCQ6skNtMoG1KOgSVbGtpPo3mcLeGsDhNiDGP264XS6tIdckOTbzMVKkvOuKKlrUs+cqJOgANknWySdoylB3i7GU2tqItYuCdxiZDfJV0zK5zbS+6nvZbmD2RiN8o2FuKKlOqKt9eg15vRIPyUQPW969t91TelS5+pzJc9U6mQj8lED1vevbfdXI/GP4S9l4XfCbsXD5NwlvYw2W2MguTsxZeivO75eRQISEthTal7CiQVJGiK7Szu9T8awjIbva7c5eLnb7dIlxbcyhS1ynkNKUhpKUgqJUoBIABJ30r4fZTwy4o3jN1G+YXlKsov70iciPIs8hMmYvZW8tDfICvXNslI6b81Zz9Tn6DPVOpn2n/JRAP/AGvevbfdT8lED1vevbfdUI+B1kOYXrgdaoWdWK6WPI7G6u0PC6x1srlIaCezeTzDa0lCkp5+oUpCzurupn6nP0GeqdTISOFFvB63a9EegzT+FQhvh1ceEkrNMxekXTiUw22iVZMfVGQ5NhLTzc7bLhPj72nR0FABQ8YnrdtKw69Rq2UaupOSs2zRYnl0XLLBZbmmPJtTl1iCazbrmgMy0I0kkLb2SCnnTvy65h6a3tRHJuFGL5fmWOZXdLYH7/jy1rt81Lq0Kb5kkFJCSApPXelA9R9O4ujOsi4T2DMch4r3G0DG4VwCrbOssR9TqIbiwE9u2Ao7QVpTtO+iSTvy1ARlrUrEtN1iX21w7lb5CJcCYyiRHfbO0uNrSFJUPmIINZdAKUpQClKUApSlAKUpQClKUApStdkUOfccfucS1XDvTdH4rrUSeWg73M8pBCHeRXRXKohXKeh1qgNPlfE/FsIv2PWS93liBdsgk9y2yGoKU5Jc6dAEg6HUDmOhsgb61FE8O73xPtuXWTizBsF2xiXc0OWi2W7tgpEZpYUgvrJBK1FCVEJ6DagSQdDbcH7Vb3MCxx45UxxIm29t5hGWL7J1x9ZWUvci0bCRtPJoKJ0gAlRG6n1AeECDGtcGPChx2okOO2llmOwgIbaQkaSlKR0AAAAA8mq96UoBSlKAVztxV/XO4F/Vt9/gIq7c1zexcOsZnZBklzYtFnho53pUhWgPQAPKpRPQJAJJIABNc/cM2co+EPxqx3i9MtBxLBsfiy42PRJyD3wuqZCORUhxO9NNkAFI6k684INAdO0pSgFKUoBX8PMtyGltOoS40tJStCxtKgehBHnFf3SgITcOGjknibZMvi5Nebcxb4a4L2Px3ki3S2yFchW1rotJVsKHXxUjyVrMR4sTW8Zm3PiTZGOGrse6G2ti43JlxiVzEdk426CBpfNrR84V6KsmtJmOE2DiFYX7Lktoh3u1PEFcSa0HEbHkUN+RQ8xGiPMaA3SVBSQpJBBGwR56/ahdns+YQuJ13kyLhbvyfqtsdi2WthHLIjyUqV2i1fmwOUpIAHOf0R4o67mlAKUpQClKUApSlAKUpQClKr/jtw8u/E7hndbLj2RXHFch5Q/brpbZz0RTb6fIlamlAqbUCUqB5gObmA5kpIAhOBcXOFPC685bgYet/Dlmw3H+TvtwajNzXH0B5brBdc2pPjpJAPTmT0GxV2225RLzbos+BKZnQJTSH48qM4HGnm1AKStCgSFJIIII6EGvgResEv8Aj2bv4jdLa/EyFiWILkF0eOHSQEgHyEHYII2CCCCQa+8+F40xheHWLH4vWNaYDEBr9xptKE/4JFAbmlKUAqvuMvHDGuB+PNXC+vOyJ0tfYW2zwUdrNuLx0A2y2OpOyAT5BsbPUAxvjZ8Ihnh9c4uH4nbFZnxNuaf9Bx+Krowk/wDTyl+Rpob31IJ+YbUMTg18HZ7HMic4gcQronMuJ0xHKq4LT/otsbO/zENs/oJGyObQUevk5lAgRnCuCGS8asmhcQONzLYRGX21iwBC+0hWsfsuSfM8/ry76D0eRKOlQNDQ6Cv2lAKUpQClKUApSlAKUqvuP/C1HGrg3lOGF4R3bnF1HdUSEpfbWl1kq1+z2jaN/NugNVj9rwlr4SGVz4V4mvZ67ZIjdwta0ER2YgWeycSezAKid7/OK+gVa1fAzD+F9+zLidAwKLDW1kEq4d7lsOJ6sOJUUuFYHkCAlRV6Ak190uH2D2zhphFkxazt9nbbTFRFZ2BzK5R1WrXlUo7UT5ySaAkNKUoBSlKAUpSgFYtyucWzwXZk19EaM0AVuLOgNnQHzkkgADqSQB1rKqqp1yOX3ldwcPPb4bq2re1vaCR4q3yP6xPMlJ8yPJrnVuSMU05S3L8sWKNF1pZKNrK4j3OYom0WRKY+tpfuj5YKuvmaSlSh6fG5T81Yvhnl3xay/aerzpTPpboL1OysHRS3FR8TeBzXFLihiGe3GDaol+x6U3J7SIVpE4NqC2kPbSdhKwCCOutp841cHhnl3xayfaeryW4lpPMtQQnYG1HQ2Tof41jSbtBhzocKRMjsTJhWI0dx1KXHylPMrkSTtWh1OvIOtM++ldjOi0eRneGeXfFrJ9p6tPl+ScQ7ri9zhWORZLPd5DKm41y5XHe5lH9sNqGlHy62dA6J3rR2lKZ/nFdholHkRz4OWA4rwtivwENTF5rdFF+6Xu9KDsu7PaKlKD2yFJHjENg7SASQeqjeNVVOgM3KMph9JKCQoKSSlSFA7StKh1SpJAIUNEEAggipdgeQv3mBIizlhdzt7nYvLAA7VJG23dDoOZPlAAAUFADWqy8mcXKKtbevmvz7czE4bNe9HcSelKVEUBSlKAVG88yOZjNojvwWWHpL8tqMkSCQgc51s661JKhXFb+ZrV9axv8AeNSU7ZW0jqScYSkuCZrPC/L/AIvZPtPU8L8v+L2T7T1KVxtYVeS7Hi9bYrmuyHhfl/xeyfaep4X5f8Xsn2nqUprCryXYa2xXNdkVJj3BVWN8fb5xZixLT39ukYMmMe07BhwgB15AA2FrCQCfnWf2ult+F+X/ABeyfaepWJdbvBsUFc25TY9vhtlKVyJTqWm0lSglIKlEAbUoAekkDz01hV5LsF7Vxb2JrsjL8L8v+L2T7T1PC/L/AIvZPtPUpTWFXkuw1tiua7IeF+X/ABeyfaeqW4VfXsmxeBc5DTbL76SVttElIIUU9N9fNUSrd8Kf6P7R+65/EVXQw9eWIpyc0tjW5c7/AEO37MxdXFZede63D4ktpSlSndMO8POR7ROdZ2Xm2FqRr+sEkiqpxRCG8Xs6Ua5BDZ0QNb8QdauFSQoEEAg9CD56qK2Ql2B6RYX9hyAeVgrOy7GP8ksf2eIf9ZCql30Wlwafqvz4nUwMkpOJCuNuUScfstqi22+T7PeblNDERm021qfMlkIWpTbTbviJ0BzFa/FSEnetg1V1j405tccKh2ZbqYeXS8xcxRN0uMNtK2G0tduX3GG1lsuhvxQlKuQq0fJV157w5hZ6bS+7cLhZ7naZCpEG52p1Lb7ClIKFgc6VJKVJUQQUmou18HLGUY9dbQufenkT7oi9iY7O3Kiz0oSnull3l5krPKCd7HUgAA6qqdGUZuV0QnjliOUQMHxuNcc8nXN9eX2nspne6Iy4gKktpTsJb5FFC/HHijegFBQ3ve5vNu2GcVOGndl5dvkTuG590IlQInauOMxlLLyHEtBTa1ghJDZSkhIGup3J5XBKDdcPn4/eMkyO9iVIZlouM6agyorzSkqbWyUoShBSpIPROid73s1smeF0M3HFLhOu91u8/HBLDEmc40pUjuhPIvtuVsA6T0TyhOtDe6DIle68OPiVNYuIWdwMd4d55dsiYuFty65QosjHUQWm2YbUwkMll0DtCtsqRzc5UFeN0HSsSycRM8awzHs4mZSJsSRlPeaRZjbmENLiquK4gVzhPP2qeigQQnQAKSdk2NYfg749YLraX0XK9zLVZ5KplqsMyYFwIDx5uVTaOQKPLzK5QtSgnfQCtg1wSsbODQcVTLuBt0O7C8tulxvtS8Jhl8pPJrk7Qka1vl6b31rJqoVOfnx2Fg17YQtSOIFxQn9By2Mqc16UuuBO/tLrxUoJBJIAHUk+atvw0t63k3C/OApTcChuKCd7jt83IsfvqWtQ9KSn6BYo7Izk91rd3+P+iPGSSpWfEm9KUqM4ApSlAKhXFb+ZrV9axv8AeNTWoVxW/ma1fWsb/eNSU/1d/QirftS+D9DWUrW5DJu0S1uO2SBFudwBTyRpktUVtQ31JcS24RobP6J383lqJDIOJnXeE4383/Oh7/ga8qk2fN4wcldW7o3vEXLRgWA5HkhY7q7029+aGN67QtoKgnfm2RrdU5w1yvizPyLGZdxg3m4WW5+NcxcINtjRIja2ipLkZbEhbpAXyjlcCiUqJ2CKslmRmmQqVa8iwzH27HMQuPNLV/dkqLSkkKHZmGgK2DrRUOhrywLg3F4eTIyoWT5NOtsNlUeHaLjcA7EjNnQCUpCApQSAAnnUrlHkqRNRi095Zi404OMkrv8Av03FTYDxFz3wQ4T5heMpF3Zyi5s2qdaTbmGWkpcS6EuoWhIWHAptJPXlOzpKelaHiTe8u4ncFb5mz+Rpg409emY8TG2oDRSY7N0bYCnXiO0DpW3znR5QOmuuxe1u4IWK2YdhuNtS7iqDis9m4QnFuNl1xxvn5Q6eTRT+cVvlCT0HUVobx8GPH7r31jNX/JLZZblOTcXrHCmoTCEgOpdK0oU2opClp2U83LskgA61Ipwyr/Isxr0VPKStt5cL+viXBSoLLvvEZuW8mNhuPPxkrUGnXMleQpad9CUiEeUka6bOvSa8RkHE35EY3/8AKXv+Aqvkv8Zz83Lw7r6lgVu+FP8AR/aP3XP4iqjVrdmP26M5cI7MScpsF5iO8Xm2166pSspSVAenlG/QKkvCn+j+0fuufxFV2MB+1U+MfSR6L2Jvqf18yW0pSrx6kVpcmxWNkrLKlrXFnRiVRpjX6bROuZJ8ykK0OZJ6HQPRSUkbqlbRk4u6MpuLuirJVtyS0KKJNmVdEJHSVa3EaV187bigpP0Aq+msXvhP+Tl69l//AFVu0qTKpvfDs2X1jaqW2xUXfCf8nL17J76d8J/ycvXsnvq3aUyqXR5mdOqckVF3wn/Jy9eye+v6bk3V88rGMXha/MFtttD+9awKtulMql0ebGnVOSK+teBTbysLyIMswOh71MLK+1+Z5fQKT6WwNHWlFSSU1YNKVpKbls3LkUqlSVV3kxSlK0IxSlKAVDuKMSVKsUJUSG/OWxcGHltR08y+RKupAqY0reEsmVzWUVJOL4lS9853ycvfsnvp3znfJy9+ye+rapVbRsN0PucfVGF8e/2Kl75zvk5e/ZPfTvnO+Tl79k99W1SmjYbofcaowvj3+xUvfOd8nL37J76d853ycvfsnvq2qU0bDdD7jVGF8e/2Kl75zvk5e/ZPfTvnO+Tl79k99W1SmjYbofcaowvj3+xUvfOd8nL37J76mfDWDJt2D2qPLYciyEoUVsujSk7WogEfQRUnpU0I06UXGnG17cb7r/Uu4bB0sLfN328xSlKF0//Z",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(part_2_graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbae0996-bb00-4d58-9d73-050d62bbf2c4",
   "metadata": {},
   "source": [
    "#### Example Conversation\n",
    "\n",
    "Now it's time to try out our newly revised chatbot! Let's run it over the following list of dialog turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "72fceb01-b0ab-4bef-a22f-a2fce6ee33ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi there, what time is my flight?\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type TavilySearchResults is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m tutorial_questions:\n\u001b[1;32m     22\u001b[0m     events \u001b[38;5;241m=\u001b[39m part_2_graph\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m     23\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, question)}, config, stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m     )\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m events:\n\u001b[1;32m     26\u001b[0m         _print_event(event, _printed)\n\u001b[1;32m     27\u001b[0m     snapshot \u001b[38;5;241m=\u001b[39m part_2_graph\u001b[38;5;241m.\u001b[39mget_state(config)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langgraph/pregel/__init__.py:845\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m    838\u001b[0m done, inflight \u001b[38;5;241m=\u001b[39m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mwait(\n\u001b[1;32m    839\u001b[0m     futures,\n\u001b[1;32m    840\u001b[0m     return_when\u001b[38;5;241m=\u001b[39mconcurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFIRST_EXCEPTION,\n\u001b[1;32m    841\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m    842\u001b[0m )\n\u001b[1;32m    844\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m--> 845\u001b[0m _panic_or_proceed(done, inflight, step)\n\u001b[1;32m    847\u001b[0m \u001b[38;5;66;03m# combine pending writes from all tasks\u001b[39;00m\n\u001b[1;32m    848\u001b[0m pending_writes \u001b[38;5;241m=\u001b[39m deque[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1370\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(done, inflight, step)\u001b[0m\n\u001b[1;32m   1368\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m   1369\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m-> 1370\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   1371\u001b[0m         \u001b[38;5;66;03m# TODO this is where retry of an entire step would happen\u001b[39;00m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langchain_core/runnables/base.py:2499\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2498\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2499\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m   2500\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2501\u001b[0m             \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[1;32m   2502\u001b[0m             patch_config(\n\u001b[1;32m   2503\u001b[0m                 config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2504\u001b[0m             ),\n\u001b[1;32m   2505\u001b[0m         )\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langgraph/utils.py:88\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     82\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, config)\n\u001b[1;32m     83\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     84\u001b[0m         {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m: config}\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m accepts_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs\n\u001b[1;32m     87\u001b[0m     )\n\u001b[0;32m---> 88\u001b[0m     ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[44], line 24\u001b[0m, in \u001b[0;36mAssistant.__call__\u001b[0;34m(self, state, config)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     passenger_id \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassenger_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 24\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunnable\u001b[38;5;241m.\u001b[39minvoke(state)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# If the LLM happens to return an empty response, we will re-prompt it\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# for an actual response.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39mtool_calls \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39mcontent[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m     ):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langchain_core/runnables/base.py:2499\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2498\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2499\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m   2500\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2501\u001b[0m             \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[1;32m   2502\u001b[0m             patch_config(\n\u001b[1;32m   2503\u001b[0m                 config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2504\u001b[0m             ),\n\u001b[1;32m   2505\u001b[0m         )\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langchain_core/runnables/base.py:4525\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4520\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4522\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4523\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4524\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m   4526\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   4527\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[1;32m   4528\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   4529\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:158\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    154\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    155\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    157\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 158\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[1;32m    159\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    160\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    161\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    162\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    163\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    164\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    165\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    166\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    167\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    168\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:560\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    554\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    558\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    559\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:421\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    420\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 421\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    422\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    423\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    425\u001b[0m ]\n\u001b[1;32m    426\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:411\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 411\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[1;32m    412\u001b[0m                 m,\n\u001b[1;32m    413\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    414\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    415\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    416\u001b[0m             )\n\u001b[1;32m    417\u001b[0m         )\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:632\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 632\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m    633\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    634\u001b[0m         )\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    636\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langchain_experimental/llms/ollama_functions.py:304\u001b[0m, in \u001b[0;36mOllamaFunctions._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m     functions \u001b[38;5;241m=\u001b[39m [convert_to_ollama_tool(fn) \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m functions]\n\u001b[1;32m    300\u001b[0m system_message_prompt_template \u001b[38;5;241m=\u001b[39m SystemMessagePromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtool_system_prompt_template\n\u001b[1;32m    302\u001b[0m )\n\u001b[1;32m    303\u001b[0m system_message \u001b[38;5;241m=\u001b[39m system_message_prompt_template\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m--> 304\u001b[0m     tools\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mdumps(functions, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    305\u001b[0m )\n\u001b[1;32m    306\u001b[0m response_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m    307\u001b[0m     [system_message] \u001b[38;5;241m+\u001b[39m messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    308\u001b[0m )\n\u001b[1;32m    309\u001b[0m chat_generation_content \u001b[38;5;241m=\u001b[39m response_message\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/json/__init__.py:238\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    235\u001b[0m     skipkeys\u001b[38;5;241m=\u001b[39mskipkeys, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii,\n\u001b[1;32m    236\u001b[0m     check_circular\u001b[38;5;241m=\u001b[39mcheck_circular, allow_nan\u001b[38;5;241m=\u001b[39mallow_nan, indent\u001b[38;5;241m=\u001b[39mindent,\n\u001b[1;32m    237\u001b[0m     separators\u001b[38;5;241m=\u001b[39mseparators, default\u001b[38;5;241m=\u001b[39mdefault, sort_keys\u001b[38;5;241m=\u001b[39msort_keys,\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\u001b[38;5;241m.\u001b[39mencode(obj)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/json/encoder.py:202\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    200\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterencode(o, _one_shot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 202\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(chunks)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/json/encoder.py:430\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m _floatstr(o)\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 430\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/json/encoder.py:326\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_list\u001b[0;34m(lst, _current_indent_level)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 326\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    328\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/json/encoder.py:439\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCircular reference detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    438\u001b[0m     markers[markerid] \u001b[38;5;241m=\u001b[39m o\n\u001b[0;32m--> 439\u001b[0m o \u001b[38;5;241m=\u001b[39m _default(o)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/json/encoder.py:180\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    181\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type TavilySearchResults is not JSON serializable"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import uuid\n",
    "\n",
    "# Update with the backup file so we can restart from the original place in each section\n",
    "shutil.copy(backup_file, db)\n",
    "thread_id = str(uuid.uuid4())\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # The passenger_id is used in our flight tools to\n",
    "        # fetch the user's flight information\n",
    "        \"passenger_id\": \"3442 587242\",\n",
    "        # Checkpoints are accessed by thread_id\n",
    "        \"thread_id\": thread_id,\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "_printed = set()\n",
    "# We can reuse the tutorial questions from part 1 to see how it does.\n",
    "for question in tutorial_questions:\n",
    "    events = part_2_graph.stream(\n",
    "        {\"messages\": (\"user\", question)}, config, stream_mode=\"values\"\n",
    "    )\n",
    "    for event in events:\n",
    "        _print_event(event, _printed)\n",
    "    snapshot = part_2_graph.get_state(config)\n",
    "    while snapshot.next:\n",
    "        # We have an interrupt! The agent is\n",
    "        # trying to use a tool.\n",
    "        # The user can approve or deny it\n",
    "        user_input = input(\n",
    "            \"Do you approve of the above actions? Type 'y' to continue;\"\n",
    "            \" otherwise, explain your requested changed.\\n\\n\"\n",
    "        )\n",
    "        if user_input.strip() == \"y\":\n",
    "            # Just continue\n",
    "            result = part_2_graph.invoke(\n",
    "                None,\n",
    "                config,\n",
    "            )\n",
    "        else:\n",
    "            # Satisfy the tool invocation by\n",
    "            # providing instructions on the requested changes / change of mind\n",
    "            result = part_2_graph.invoke(\n",
    "                {\n",
    "                    \"messages\": [\n",
    "                        ToolMessage(\n",
    "                            tool_call_id=event[\"messages\"][-1].tool_calls[0][\"id\"],\n",
    "                            content=f\"API call denied by user. Reasoning: '{user_input}'. Continue assisting, accounting for the user's input.\",\n",
    "                        )\n",
    "                    ]\n",
    "                },\n",
    "                config,\n",
    "            )\n",
    "        snapshot = part_2_graph.get_state(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a3a805-e39b-4ec5-87c7-18a9e86c0428",
   "metadata": {},
   "source": [
    "#### Part 2 Review\n",
    "\n",
    "Now our assistant was able to save a step to respond with our flight details. We also completely controlled which actions were performed. This all worked using LangGraph's `interrupts` and `checkpointers`. The interrupt pauses graph execution, its state safely persisted using your configured checkpointer. The user can then start it up at any time by running it with the right config.\n",
    "\n",
    "See an [example LangSmith trace](https://smith.langchain.com/public/b3c71814-c366-476d-be6a-f6f3056caaec/r) to get a better sense of how the graph is running. Note [from this trace](https://smith.langchain.com/public/a077f4be-6baa-4e97-89f7-0dabc65c0fd0/r) that you typically **resume** a flow by invoking the graph with `(None, config)`. The state is loaded from the checkpoint as if it never was interrupted.\n",
    "\n",
    "This graph worked pretty well! We *didn't really* need to be involved in *EVERY* assistant action, though...\n",
    "\n",
    "In the next section, we will reorganize our graph so that we can interrupt only on the \"sensitive\" actions that actually write to the database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f656c4e-b18b-43f4-ba35-4eca5693444d",
   "metadata": {},
   "source": [
    "## Part 3: Conditional Interrupt\n",
    "\n",
    "In this section, we'll refine our interrupt strategy by categorizing tools as safe (read-only) or sensitive (data-modifying). We'll apply interrupts to the sensitive tools only, allowing the bot to handle simple queries autonomously.\n",
    "\n",
    "This balances user control and conversational flow, but as we add more tools, our single graph may grow too complex for this \"flat\" structure. We'll address that in the next section. \n",
    "\n",
    "Your graph for Part 3 will look something like the following diagram.\n",
    "\n",
    "![Part 3 Diagram](../img/part-3-diagram.png)\n",
    "\n",
    "\n",
    "#### State\n",
    "\n",
    "As always, start by defining the graph state. Our state and LLM calling **are identical to** part 2. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "20f99193-9195-42ae-8df1-0cf1489a164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable, RunnableConfig\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    user_info: str\n",
    "\n",
    "\n",
    "class Assistant:\n",
    "    def __init__(self, runnable: Runnable):\n",
    "        self.runnable = runnable\n",
    "\n",
    "    def __call__(self, state: State, config: RunnableConfig):\n",
    "        while True:\n",
    "            passenger_id = config.get(\"passenger_id\", None)\n",
    "            result = self.runnable.invoke(state)\n",
    "            # If the LLM happens to return an empty response, we will re-prompt it\n",
    "            # for an actual response.\n",
    "            if not result.tool_calls and (\n",
    "                not result.content\n",
    "                or isinstance(result.content, list)\n",
    "                and not result.content[0].get(\"text\")\n",
    "            ):\n",
    "                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n",
    "                state = {**state, \"messages\": messages}\n",
    "                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n",
    "                state = {**state, \"messages\": messages}\n",
    "            else:\n",
    "                break\n",
    "        return {\"messages\": result}\n",
    "\n",
    "\n",
    "# Haiku is faster and cheaper, but less accurate\n",
    "# llm = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "#llm = ChatAnthropic(model=\"claude-3-sonnet-20240229\", temperature=1)\n",
    "\n",
    "\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "llm = OllamaFunctions(model=\"llama3\")\n",
    "\n",
    "# You can update the LLMs, though you may need to update the prompts\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")\n",
    "\n",
    "assistant_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful customer support assistant for Swiss Airlines. \"\n",
    "            \" Use the provided tools to search for flights, company policies, and other information to assist the user's queries. \"\n",
    "            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n",
    "            \" If a search comes up empty, expand your search before giving up.\"\n",
    "            \"\\n\\nCurrent user:\\n<User>\\n{user_info}\\n</User>\"\n",
    "            \"\\nCurrent time: {time}.\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ").partial(time=datetime.now())\n",
    "\n",
    "\n",
    "# \"Read\"-only tools (such as retrievers) don't need a user confirmation to use\n",
    "part_3_safe_tools = [\n",
    "    TavilySearchResults(max_results=1),\n",
    "    fetch_user_flight_information,\n",
    "    search_flights,\n",
    "    lookup_policy,\n",
    "    search_car_rentals,\n",
    "    search_hotels,\n",
    "    search_trip_recommendations,\n",
    "]\n",
    "\n",
    "# These tools all change the user's reservations.\n",
    "# The user has the right to control what decisions are made\n",
    "part_3_sensitive_tools = [\n",
    "    update_ticket_to_new_flight,\n",
    "    cancel_ticket,\n",
    "    book_car_rental,\n",
    "    update_car_rental,\n",
    "    cancel_car_rental,\n",
    "    book_hotel,\n",
    "    update_hotel,\n",
    "    cancel_hotel,\n",
    "    book_excursion,\n",
    "    update_excursion,\n",
    "    cancel_excursion,\n",
    "]\n",
    "sensitive_tool_names = {t.name for t in part_3_sensitive_tools}\n",
    "# Our LLM doesn't have to know which nodes it has to route to. In its 'mind', it's just invoking functions.\n",
    "part_3_assistant_runnable = assistant_prompt | llm.bind_tools(\n",
    "    part_3_safe_tools + part_3_sensitive_tools\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1677dd5-4cbe-4d87-bdbf-5d179eb6acae",
   "metadata": {},
   "source": [
    "#### Define Graph\n",
    "\n",
    "Now, create the graph. Our graph is almost identical to part 2 **except** we split out the tools into 2 separate nodes. We only interrupt before the tools that are actually making changes to the user's bookings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "928b756f-2934-4b1b-95d1-0c4f974b978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "\n",
    "def user_info(state: State):\n",
    "    return {\"user_info\": fetch_user_flight_information.invoke({})}\n",
    "\n",
    "\n",
    "# NEW: The fetch_user_info node runs first, meaning our assistant can see the user's flight information without\n",
    "# having to take an action\n",
    "builder.add_node(\"fetch_user_info\", user_info)\n",
    "builder.set_entry_point(\"fetch_user_info\")\n",
    "builder.add_node(\"assistant\", Assistant(part_3_assistant_runnable))\n",
    "builder.add_node(\"safe_tools\", create_tool_node_with_fallback(part_3_safe_tools))\n",
    "builder.add_node(\n",
    "    \"sensitive_tools\", create_tool_node_with_fallback(part_3_sensitive_tools)\n",
    ")\n",
    "# Define logic\n",
    "builder.add_edge(\"fetch_user_info\", \"assistant\")\n",
    "\n",
    "\n",
    "def route_tools(state: State) -> Literal[\"safe_tools\", \"sensitive_tools\", \"__end__\"]:\n",
    "    next_node = tools_condition(state)\n",
    "    # If no tools are invoked, return to the user\n",
    "    if next_node == END:\n",
    "        return END\n",
    "    ai_message = state[\"messages\"][-1]\n",
    "    # This assumes single tool calls. To handle parallel tool calling, you'd want to\n",
    "    # use an ANY condition\n",
    "    first_tool_call = ai_message.tool_calls[0]\n",
    "    if first_tool_call[\"name\"] in sensitive_tool_names:\n",
    "        return \"sensitive_tools\"\n",
    "    return \"safe_tools\"\n",
    "\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    route_tools,\n",
    ")\n",
    "builder.add_edge(\"safe_tools\", \"assistant\")\n",
    "builder.add_edge(\"sensitive_tools\", \"assistant\")\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "part_3_graph = builder.compile(\n",
    "    checkpointer=memory,\n",
    "    # NEW: The graph will always halt before executing the \"action\" node.\n",
    "    # The user can approve or reject (or even alter the request) before\n",
    "    # the assistant continues\n",
    "    interrupt_before=[\"sensitive_tools\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "35e0e314-0df8-4d73-800c-f8edd5e3ef39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(part_3_graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb2b129-d5a7-450c-a119-c08b93757432",
   "metadata": {},
   "source": [
    "#### Example Conversation\n",
    "\n",
    "Now it's time to try out our newly revised chatbot! Let's run it over the following list of dialog turns. This time, we'll have many fewer confirmations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "96469e95-5070-4169-bedd-45db94b43d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi there, what time is my flight?\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type TavilySearchResults is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 42\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m tutorial_questions:\n\u001b[1;32m     39\u001b[0m     events \u001b[38;5;241m=\u001b[39m part_3_graph\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m     40\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, question)}, config, stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m     )\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m events:\n\u001b[1;32m     43\u001b[0m         _print_event(event, _printed)\n\u001b[1;32m     44\u001b[0m     snapshot \u001b[38;5;241m=\u001b[39m part_3_graph\u001b[38;5;241m.\u001b[39mget_state(config)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langgraph/pregel/__init__.py:845\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m    838\u001b[0m done, inflight \u001b[38;5;241m=\u001b[39m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mwait(\n\u001b[1;32m    839\u001b[0m     futures,\n\u001b[1;32m    840\u001b[0m     return_when\u001b[38;5;241m=\u001b[39mconcurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFIRST_EXCEPTION,\n\u001b[1;32m    841\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m    842\u001b[0m )\n\u001b[1;32m    844\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m--> 845\u001b[0m _panic_or_proceed(done, inflight, step)\n\u001b[1;32m    847\u001b[0m \u001b[38;5;66;03m# combine pending writes from all tasks\u001b[39;00m\n\u001b[1;32m    848\u001b[0m pending_writes \u001b[38;5;241m=\u001b[39m deque[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1370\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(done, inflight, step)\u001b[0m\n\u001b[1;32m   1368\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m   1369\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m-> 1370\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   1371\u001b[0m         \u001b[38;5;66;03m# TODO this is where retry of an entire step would happen\u001b[39;00m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langchain_core/runnables/base.py:2499\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2498\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2499\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m   2500\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2501\u001b[0m             \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[1;32m   2502\u001b[0m             patch_config(\n\u001b[1;32m   2503\u001b[0m                 config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2504\u001b[0m             ),\n\u001b[1;32m   2505\u001b[0m         )\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langgraph/utils.py:88\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     82\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, config)\n\u001b[1;32m     83\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     84\u001b[0m         {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m: config}\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m accepts_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs\n\u001b[1;32m     87\u001b[0m     )\n\u001b[0;32m---> 88\u001b[0m     ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[48], line 24\u001b[0m, in \u001b[0;36mAssistant.__call__\u001b[0;34m(self, state, config)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     passenger_id \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassenger_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 24\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunnable\u001b[38;5;241m.\u001b[39minvoke(state)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# If the LLM happens to return an empty response, we will re-prompt it\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# for an actual response.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39mtool_calls \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39mcontent[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m     ):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langchain_core/runnables/base.py:2499\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2498\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2499\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m   2500\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2501\u001b[0m             \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[1;32m   2502\u001b[0m             patch_config(\n\u001b[1;32m   2503\u001b[0m                 config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2504\u001b[0m             ),\n\u001b[1;32m   2505\u001b[0m         )\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langchain_core/runnables/base.py:4525\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4520\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4522\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4523\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4524\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m   4526\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   4527\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[1;32m   4528\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   4529\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:158\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    154\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    155\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    157\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 158\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[1;32m    159\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    160\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    161\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    162\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    163\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    164\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    165\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    166\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    167\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    168\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:560\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    554\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    558\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    559\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:421\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    420\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 421\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    422\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    423\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    425\u001b[0m ]\n\u001b[1;32m    426\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:411\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 411\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[1;32m    412\u001b[0m                 m,\n\u001b[1;32m    413\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    414\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    415\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    416\u001b[0m             )\n\u001b[1;32m    417\u001b[0m         )\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:632\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 632\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m    633\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    634\u001b[0m         )\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    636\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/site-packages/langchain_experimental/llms/ollama_functions.py:304\u001b[0m, in \u001b[0;36mOllamaFunctions._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m     functions \u001b[38;5;241m=\u001b[39m [convert_to_ollama_tool(fn) \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m functions]\n\u001b[1;32m    300\u001b[0m system_message_prompt_template \u001b[38;5;241m=\u001b[39m SystemMessagePromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtool_system_prompt_template\n\u001b[1;32m    302\u001b[0m )\n\u001b[1;32m    303\u001b[0m system_message \u001b[38;5;241m=\u001b[39m system_message_prompt_template\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m--> 304\u001b[0m     tools\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mdumps(functions, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    305\u001b[0m )\n\u001b[1;32m    306\u001b[0m response_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m    307\u001b[0m     [system_message] \u001b[38;5;241m+\u001b[39m messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    308\u001b[0m )\n\u001b[1;32m    309\u001b[0m chat_generation_content \u001b[38;5;241m=\u001b[39m response_message\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/json/__init__.py:238\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    235\u001b[0m     skipkeys\u001b[38;5;241m=\u001b[39mskipkeys, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii,\n\u001b[1;32m    236\u001b[0m     check_circular\u001b[38;5;241m=\u001b[39mcheck_circular, allow_nan\u001b[38;5;241m=\u001b[39mallow_nan, indent\u001b[38;5;241m=\u001b[39mindent,\n\u001b[1;32m    237\u001b[0m     separators\u001b[38;5;241m=\u001b[39mseparators, default\u001b[38;5;241m=\u001b[39mdefault, sort_keys\u001b[38;5;241m=\u001b[39msort_keys,\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\u001b[38;5;241m.\u001b[39mencode(obj)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/json/encoder.py:202\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    200\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterencode(o, _one_shot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 202\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(chunks)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/json/encoder.py:430\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m _floatstr(o)\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 430\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/json/encoder.py:326\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_list\u001b[0;34m(lst, _current_indent_level)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 326\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    328\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/json/encoder.py:439\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCircular reference detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    438\u001b[0m     markers[markerid] \u001b[38;5;241m=\u001b[39m o\n\u001b[0;32m--> 439\u001b[0m o \u001b[38;5;241m=\u001b[39m _default(o)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/langraph/lib/python3.11/json/encoder.py:180\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    181\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type TavilySearchResults is not JSON serializable"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import uuid\n",
    "\n",
    "# Update with the backup file so we can restart from the original place in each section\n",
    "shutil.copy(backup_file, db)\n",
    "thread_id = str(uuid.uuid4())\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # The passenger_id is used in our flight tools to\n",
    "        # fetch the user's flight information\n",
    "        \"passenger_id\": \"3442 587242\",\n",
    "        # Checkpoints are accessed by thread_id\n",
    "        \"thread_id\": thread_id,\n",
    "    }\n",
    "}\n",
    "\n",
    "tutorial_questions = [\n",
    "    \"Hi there, what time is my flight?\",\n",
    "    \"Am i allowed to update my flight to something sooner? I want to leave later today.\",\n",
    "    \"Update my flight to sometime next week then\",\n",
    "    \"The next available option is great\",\n",
    "    \"what about lodging and transportation?\",\n",
    "    \"Yeah i think i'd like an affordable hotel for my week-long stay (7 days). And I'll want to rent a car.\",\n",
    "    \"OK could you place a reservation for your recommended hotel? It sounds nice.\",\n",
    "    \"yes go ahead and book anything that's moderate expense and has availability.\",\n",
    "    \"Now for a car, what are my options?\",\n",
    "    \"Awesome let's just get the cheapest option. Go ahead and book for 7 days\",\n",
    "    \"Cool so now what recommendations do you have on excursions?\",\n",
    "    \"Are they available while I'm there?\",\n",
    "    \"interesting - i like the museums, what options are there? \",\n",
    "    \"OK great pick one and book it for my second day there.\",\n",
    "]\n",
    "\n",
    "\n",
    "_printed = set()\n",
    "# We can reuse the tutorial questions from part 1 to see how it does.\n",
    "for question in tutorial_questions:\n",
    "    events = part_3_graph.stream(\n",
    "        {\"messages\": (\"user\", question)}, config, stream_mode=\"values\"\n",
    "    )\n",
    "    for event in events:\n",
    "        _print_event(event, _printed)\n",
    "    snapshot = part_3_graph.get_state(config)\n",
    "    while snapshot.next:\n",
    "        # We have an interrupt! The agent is\n",
    "        # trying to use a tool.\n",
    "        # The user can approve or deny it\n",
    "        user_input = input(\n",
    "            \"Do you approve of the above actions? Type 'y' to continue;\"\n",
    "            \" otherwise, explain your requested changed.\\n\\n\"\n",
    "        )\n",
    "        if user_input.strip() == \"y\":\n",
    "            # Just continue\n",
    "            result = part_3_graph.invoke(\n",
    "                None,\n",
    "                config,\n",
    "            )\n",
    "        else:\n",
    "            # Satisfy the tool invocation by\n",
    "            # providing instructions on the requested changes / change of mind\n",
    "            result = part_3_graph.invoke(\n",
    "                {\n",
    "                    \"messages\": [\n",
    "                        ToolMessage(\n",
    "                            tool_call_id=event[\"messages\"][-1].tool_calls[0][\"id\"],\n",
    "                            content=f\"API call denied by user. Reasoning: '{user_input}'. Continue assisting, accounting for the user's input.\",\n",
    "                        )\n",
    "                    ]\n",
    "                },\n",
    "                config,\n",
    "            )\n",
    "        snapshot = part_3_graph.get_state(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af6695d-f5f1-44e8-a90f-87a720c29700",
   "metadata": {},
   "source": [
    "#### Part 3 Review\n",
    "\n",
    "Much better! Our agent is now working well - [check out a LangSmith trace](https://smith.langchain.com/public/a0d64d8b-1714-4cfe-a239-e170ca45e81a/r) of our latest run to inspect its work! You may be satisfied with this design. The code is contained, and it's behaving as desired. \n",
    "\n",
    "One problem with this design is that we're putting a lot of pressure on a single prompt. If we want to add more tools, or if each tool gets more complicated (more filters, more business logic constraining behavior, etc), it's likely the tool usage and overall behavior of the bot will start to suffer. \n",
    "\n",
    "In the next section, we show how you can take more control over different user experiences by routing to specialist agents or sub-graphs based on the user's intent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcee294d-637a-4783-82ff-751cf6e9fbdb",
   "metadata": {},
   "source": [
    "## Part 4: Specialized Workflows\n",
    "\n",
    "In the previous sections, we saw how \"wide\" chat-bots, relying on a single prompt and LLM to handle various user intents, can get us far. However, it's difficult to create **predictably great** user experiences for known intents with this approach.\n",
    "\n",
    "Alternatively, your graph can detect userintent and select the appropriate workflow or \"skill\" to satisfy the user's needs. Each workflow can focus on its domain, allowing for isolated improvements without degrading the overall assistant.\n",
    "\n",
    "In this section, we'll split user experiences into separate sub-graphs, resulting in a structure like this:\n",
    "\n",
    "![Part 4 Diagram](../img/part-4-diagram.png)\n",
    "\n",
    "In the diagram above, each square wraps an agentic, focused workflow. The primary assistant fields the user's initial queries, and the graph routes to the appropriate \"expert\" based on the query content.\n",
    "\n",
    "#### State\n",
    "\n",
    "We want to keep track of which sub-graph is in control at any given moment. While we _could_ do this through some arithmetic on the message list, it's easier to track as a dedicated **stack**. \n",
    "\n",
    "Add a `dialog_state` list to the `State` below. Any time a `node` is run and returns a value for `dialog_state`, the `update_dialog_stack` function will be called to determine how to apply the update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2997e1f9-3a4b-4794-b71f-992da3a644fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal, Optional\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "\n",
    "\n",
    "def update_dialog_stack(left: list[str], right: Optional[str]) -> list[str]:\n",
    "    \"\"\"Push or pop the state.\"\"\"\n",
    "    if right is None:\n",
    "        return left\n",
    "    if right == \"pop\":\n",
    "        return left[:-1]\n",
    "    return left + [right]\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    user_info: str\n",
    "    dialog_state: Annotated[\n",
    "        list[\n",
    "            Literal[\n",
    "                \"assistant\",\n",
    "                \"update_flight\",\n",
    "                \"book_car_rental\",\n",
    "                \"book_hotel\",\n",
    "                \"book_excursion\",\n",
    "            ]\n",
    "        ],\n",
    "        update_dialog_stack,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67fb372-a8dc-49c8-b86a-2742e0f8aae9",
   "metadata": {},
   "source": [
    "#### Assistants\n",
    "\n",
    "This time we will create an assistant **for every workflow**. That means:\n",
    "\n",
    "1. Flight booking assistant\n",
    "2. Hotel booking assistant\n",
    "3. Car rental assistant\n",
    "4. Excursion assistant\n",
    "5. and finally, a \"primary assistant\" to route between these\n",
    "\n",
    "If you're paying attention, you may recognize this as an example of the **supervisor** design pattern from our Multi-agent examples.\n",
    "\n",
    "Below, define the `Runnable` objects to power each assistant.\n",
    "Each `Runnable` has a prompt, LLM, and schemas for the tools scoped to that assistant.\n",
    "Each *specialized* / delegated assistant additionally can call the `CompleteOrEscalate` tool to indicate that the control flow should be passed back to the primary assistant. This happens if it has successfully completed its work or if the user has changed their mind or needs assistance on something that beyond the scope of that particular workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef67c85-b999-406c-a745-09fdc0dfa0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import Runnable, RunnableConfig\n",
    "\n",
    "\n",
    "class Assistant:\n",
    "    def __init__(self, runnable: Runnable):\n",
    "        self.runnable = runnable\n",
    "\n",
    "    def __call__(self, state: State, config: RunnableConfig):\n",
    "        while True:\n",
    "            result = self.runnable.invoke(state)\n",
    "\n",
    "            if not result.tool_calls and (\n",
    "                not result.content\n",
    "                or isinstance(result.content, list)\n",
    "                and not result.content[0].get(\"text\")\n",
    "            ):\n",
    "                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n",
    "                state = {**state, \"messages\": messages}\n",
    "                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n",
    "                state = {**state, \"messages\": messages}\n",
    "            else:\n",
    "                break\n",
    "        return {\"messages\": result}\n",
    "\n",
    "\n",
    "class CompleteOrEscalate(BaseModel):\n",
    "    \"\"\"A tool to mark the current task as completed and/or to escalate control of the dialog to the main assistant,\n",
    "    who can re-route the dialog based on the user's needs.\"\"\"\n",
    "\n",
    "    cancel: bool = True\n",
    "    reason: str\n",
    "\n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"cancel\": True,\n",
    "                \"reason\": \"User changed their mind about the current task.\",\n",
    "            },\n",
    "            \"example 2\": {\n",
    "                \"cancel\": True,\n",
    "                \"reason\": \"I have fully completed the task.\",\n",
    "            },\n",
    "            \"example 3\": {\n",
    "                \"cancel\": False,\n",
    "                \"reason\": \"I need to search the user's emails or calendar for more information.\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "\n",
    "# Flight booking assistant\n",
    "\n",
    "flight_booking_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a specialized assistant for handling flight updates. \"\n",
    "            \" The primary assistant delegates work to you whenever the user needs help updating their bookings. \"\n",
    "            \"Confirm the updated flight details with the customer and inform them of any additional fees. \"\n",
    "            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n",
    "            \"If you need more information or the customer changes their mind, escalate the task back to the main assistant.\"\n",
    "            \" Remember that a booking isn't completed until after the relevant tool has successfully been used.\"\n",
    "            \"\\n\\nCurrent user flight information:\\n<Flights>\\n{user_info}\\n</Flights>\"\n",
    "            \"\\nCurrent time: {time}.\"\n",
    "            \"\\n\\nIf the user needs help, and none of your tools are appropriate for it, then\"\n",
    "            ' \"CompleteOrEscalate\" the dialog to the host assistant. Do not waste the user\\'s time. Do not make up invalid tools or functions.',\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ").partial(time=datetime.now())\n",
    "\n",
    "update_flight_safe_tools = [search_flights]\n",
    "update_flight_sensitive_tools = [update_ticket_to_new_flight, cancel_ticket]\n",
    "update_flight_tools = update_flight_safe_tools + update_flight_sensitive_tools\n",
    "update_flight_runnable = flight_booking_prompt | llm.bind_tools(\n",
    "    update_flight_tools + [CompleteOrEscalate]\n",
    ")\n",
    "\n",
    "# Hotel Booking Assistant\n",
    "book_hotel_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a specialized assistant for handling hotel bookings. \"\n",
    "            \"The primary assistant delegates work to you whenever the user needs help booking a hotel. \"\n",
    "            \"Search for available hotels based on the user's preferences and confirm the booking details with the customer. \"\n",
    "            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n",
    "            \"If you need more information or the customer changes their mind, escalate the task back to the main assistant.\"\n",
    "            \" Remember that a booking isn't completed until after the relevant tool has successfully been used.\"\n",
    "            \"\\nCurrent time: {time}.\"\n",
    "            '\\n\\nIf the user needs help, and none of your tools are appropriate for it, then \"CompleteOrEscalate\" the dialog to the host assistant.'\n",
    "            \" Do not waste the user's time. Do not make up invalid tools or functions.\"\n",
    "            \"\\n\\nSome examples for which you should CompleteOrEscalate:\\n\"\n",
    "            \" - 'what's the weather like this time of year?'\\n\"\n",
    "            \" - 'nevermind i think I'll book separately'\\n\"\n",
    "            \" - 'i need to figure out transportation while i'm there'\\n\"\n",
    "            \" - 'Oh wait i haven't booked my flight yet i'll do that first'\\n\"\n",
    "            \" - 'Hotel booking confirmed'\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ").partial(time=datetime.now())\n",
    "\n",
    "book_hotel_safe_tools = [search_hotels]\n",
    "book_hotel_sensitive_tools = [book_hotel, update_hotel, cancel_hotel]\n",
    "book_hotel_tools = book_hotel_safe_tools + book_hotel_sensitive_tools\n",
    "book_hotel_runnable = book_hotel_prompt | llm.bind_tools(\n",
    "    book_hotel_tools + [CompleteOrEscalate]\n",
    ")\n",
    "\n",
    "# Car Rental Assistant\n",
    "book_car_rental_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a specialized assistant for handling car rental bookings. \"\n",
    "            \"The primary assistant delegates work to you whenever the user needs help booking a car rental. \"\n",
    "            \"Search for available car rentals based on the user's preferences and confirm the booking details with the customer. \"\n",
    "            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n",
    "            \"If you need more information or the customer changes their mind, escalate the task back to the main assistant.\"\n",
    "            \" Remember that a booking isn't completed until after the relevant tool has successfully been used.\"\n",
    "            \"\\nCurrent time: {time}.\"\n",
    "            \"\\n\\nIf the user needs help, and none of your tools are appropriate for it, then \"\n",
    "            '\"CompleteOrEscalate\" the dialog to the host assistant. Do not waste the user\\'s time. Do not make up invalid tools or functions.'\n",
    "            \"\\n\\nSome examples for which you should CompleteOrEscalate:\\n\"\n",
    "            \" - 'what's the weather like this time of year?'\\n\"\n",
    "            \" - 'What flights are available?'\\n\"\n",
    "            \" - 'nevermind i think I'll book separately'\\n\"\n",
    "            \" - 'Oh wait i haven't booked my flight yet i'll do that first'\\n\"\n",
    "            \" - 'Car rental booking confirmed'\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ").partial(time=datetime.now())\n",
    "\n",
    "book_car_rental_safe_tools = [search_car_rentals]\n",
    "book_car_rental_sensitive_tools = [\n",
    "    book_car_rental,\n",
    "    update_car_rental,\n",
    "    cancel_car_rental,\n",
    "]\n",
    "book_car_rental_tools = book_car_rental_safe_tools + book_car_rental_sensitive_tools\n",
    "book_car_rental_runnable = book_car_rental_prompt | llm.bind_tools(\n",
    "    book_car_rental_tools + [CompleteOrEscalate]\n",
    ")\n",
    "\n",
    "# Excursion Assistant\n",
    "\n",
    "book_excursion_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a specialized assistant for handling trip recommendations. \"\n",
    "            \"The primary assistant delegates work to you whenever the user needs help booking a recommended trip. \"\n",
    "            \"Search for available trip recommendations based on the user's preferences and confirm the booking details with the customer. \"\n",
    "            \"If you need more information or the customer changes their mind, escalate the task back to the main assistant.\"\n",
    "            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n",
    "            \" Remember that a booking isn't completed until after the relevant tool has successfully been used.\"\n",
    "            \"\\nCurrent time: {time}.\"\n",
    "            '\\n\\nIf the user needs help, and none of your tools are appropriate for it, then \"CompleteOrEscalate\" the dialog to the host assistant. Do not waste the user\\'s time. Do not make up invalid tools or functions.'\n",
    "            \"\\n\\nSome examples for which you should CompleteOrEscalate:\\n\"\n",
    "            \" - 'nevermind i think I'll book separately'\\n\"\n",
    "            \" - 'i need to figure out transportation while i'm there'\\n\"\n",
    "            \" - 'Oh wait i haven't booked my flight yet i'll do that first'\\n\"\n",
    "            \" - 'Excursion booking confirmed!'\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ").partial(time=datetime.now())\n",
    "\n",
    "book_excursion_safe_tools = [search_trip_recommendations]\n",
    "book_excursion_sensitive_tools = [book_excursion, update_excursion, cancel_excursion]\n",
    "book_excursion_tools = book_excursion_safe_tools + book_excursion_sensitive_tools\n",
    "book_excursion_runnable = book_excursion_prompt | llm.bind_tools(\n",
    "    book_excursion_tools + [CompleteOrEscalate]\n",
    ")\n",
    "\n",
    "\n",
    "# Primary Assistant\n",
    "class ToFlightBookingAssistant(BaseModel):\n",
    "    \"\"\"Transfers work to a specialized assistant to handle flight updates and cancellations.\"\"\"\n",
    "\n",
    "    request: str = Field(\n",
    "        description=\"Any necessary followup questions the update flight assistant should clarify before proceeding.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ToBookCarRental(BaseModel):\n",
    "    \"\"\"Transfers work to a specialized assistant to handle car rental bookings.\"\"\"\n",
    "\n",
    "    location: str = Field(\n",
    "        description=\"The location where the user wants to rent a car.\"\n",
    "    )\n",
    "    start_date: str = Field(description=\"The start date of the car rental.\")\n",
    "    end_date: str = Field(description=\"The end date of the car rental.\")\n",
    "    request: str = Field(\n",
    "        description=\"Any additional information or requests from the user regarding the car rental.\"\n",
    "    )\n",
    "\n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"location\": \"Basel\",\n",
    "                \"start_date\": \"2023-07-01\",\n",
    "                \"end_date\": \"2023-07-05\",\n",
    "                \"request\": \"I need a compact car with automatic transmission.\",\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class ToHotelBookingAssistant(BaseModel):\n",
    "    \"\"\"Transfer work to a specialized assistant to handle hotel bookings.\"\"\"\n",
    "\n",
    "    location: str = Field(\n",
    "        description=\"The location where the user wants to book a hotel.\"\n",
    "    )\n",
    "    checkin_date: str = Field(description=\"The check-in date for the hotel.\")\n",
    "    checkout_date: str = Field(description=\"The check-out date for the hotel.\")\n",
    "    request: str = Field(\n",
    "        description=\"Any additional information or requests from the user regarding the hotel booking.\"\n",
    "    )\n",
    "\n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"location\": \"Zurich\",\n",
    "                \"checkin_date\": \"2023-08-15\",\n",
    "                \"checkout_date\": \"2023-08-20\",\n",
    "                \"request\": \"I prefer a hotel near the city center with a room that has a view.\",\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class ToBookExcursion(BaseModel):\n",
    "    \"\"\"Transfers work to a specialized assistant to handle trip recommendation and other excursion bookings.\"\"\"\n",
    "\n",
    "    location: str = Field(\n",
    "        description=\"The location where the user wants to book a recommended trip.\"\n",
    "    )\n",
    "    request: str = Field(\n",
    "        description=\"Any additional information or requests from the user regarding the trip recommendation.\"\n",
    "    )\n",
    "\n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"location\": \"Lucerne\",\n",
    "                \"request\": \"The user is interested in outdoor activities and scenic views.\",\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "# The top-level assistant performs general Q&A and delegates specialized tasks to other assistants.\n",
    "# The task delegation is a simple form of semantic routing / does simple intent detection\n",
    "# llm = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "llm = ChatAnthropic(model=\"claude-3-sonnet-20240229\", temperature=1)\n",
    "\n",
    "primary_assistant_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful customer support assistant for Swiss Airlines. \"\n",
    "            \"Your primary role is to search for flight information and company policies to answer customer queries. \"\n",
    "            \"If a customer requests to update or cancel a flight, book a car rental, book a hotel, or get trip recommendations, \"\n",
    "            \"delegate the task to the appropriate specialized assistant by invoking the corresponding tool. You are not able to make these types of changes yourself.\"\n",
    "            \" Only the specialized assistants are given permission to do this for the user.\"\n",
    "            \"The user is not aware of the different specialized assistants, so do not mention them; just quietly delegate through function calls. \"\n",
    "            \"Provide detailed information to the customer, and always double-check the database before concluding that information is unavailable. \"\n",
    "            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n",
    "            \" If a search comes up empty, expand your search before giving up.\"\n",
    "            \"\\n\\nCurrent user flight information:\\n<Flights>\\n{user_info}\\n</Flights>\"\n",
    "            \"\\nCurrent time: {time}.\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ").partial(time=datetime.now())\n",
    "primary_assistant_tools = [\n",
    "    TavilySearchResults(max_results=1),\n",
    "    search_flights,\n",
    "    lookup_policy,\n",
    "]\n",
    "assistant_runnable = primary_assistant_prompt | llm.bind_tools(\n",
    "    primary_assistant_tools\n",
    "    + [\n",
    "        ToFlightBookingAssistant,\n",
    "        ToBookCarRental,\n",
    "        ToHotelBookingAssistant,\n",
    "        ToBookExcursion,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6fc3e3-185f-4c1b-a2e3-cebac35ce0d6",
   "metadata": {},
   "source": [
    "#### Create Assistant\n",
    "\n",
    "We're about ready to create the graph. In the previous section, we made the design decision to have a shared `messages` state between all the nodes. This is powerful in that each delegated assistant can see the entire user journey and have a shared context. This, however, means that weaker LLMs can easily get mixed up about there specific scope. To mark the \"handoff\" between the primary assistant and one of the delegated workflows (and complete the tool call from the router), we will add a `ToolMessage` to the state.\n",
    "\n",
    "\n",
    "#### Utility\n",
    "\n",
    "Create a function to make an \"entry\" node for each workflow, stating \"the current assistant ix `assistant_name`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb812818-99c9-4bf3-b1e5-a394c7b9058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "\n",
    "def create_entry_node(assistant_name: str, new_dialog_state: str) -> Callable:\n",
    "    def entry_node(state: State) -> dict:\n",
    "        tool_call_id = state[\"messages\"][-1].tool_calls[0][\"id\"]\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                ToolMessage(\n",
    "                    content=f\"The assistant is now the {assistant_name}. Reflect on the above conversation between the host assistant and the user.\"\n",
    "                    f\" The user's intent is unsatisfied. Use the provided tools to assist the user. Remember, you are {assistant_name},\"\n",
    "                    \" and the booking, update, other other action is not complete until after you have successfully invoked the appropriate tool.\"\n",
    "                    \" If the user changes their mind or needs help for other tasks, call the CompleteOrEscalate function to let the primary host assistant take control.\"\n",
    "                    \" Do not mention who you are - just act as the proxy for the assistant.\",\n",
    "                    tool_call_id=tool_call_id,\n",
    "                )\n",
    "            ],\n",
    "            \"dialog_state\": new_dialog_state,\n",
    "        }\n",
    "\n",
    "    return entry_node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff93003-6c61-437c-9510-4eeaecfd517b",
   "metadata": {},
   "source": [
    "#### Define Graph\n",
    "\n",
    "Now it's time to start building our graph. As before, we'll start with a node to pre-populate the state with the user's current information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c1140c-cd4e-4d69-bddd-7baa1eb4540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "\n",
    "def user_info(state: State):\n",
    "    return {\"user_info\": fetch_user_flight_information.invoke({})}\n",
    "\n",
    "\n",
    "builder.add_node(\"fetch_user_info\", user_info)\n",
    "builder.set_entry_point(\"fetch_user_info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fc80d0-fbf7-4631-9a8e-6b5af966e112",
   "metadata": {},
   "source": [
    "Now we'll start adding our specialized workflows. Each mini-workflow looks very similar to our full graph in [Part 3](#part-3-conditional-interrupt), employing 5 nodes:\n",
    "\n",
    "1. `enter_*`: use the `create_entry_node` utility you defined above to add a ToolMessage signaling that the new specialized assistant is at the helm\n",
    "2. Assistant: the prompt + llm combo that takes in the current state and either uses a tool, asks a question of the user, or ends the workflow (return to the primary assistant)\n",
    "3. `*_safe_tools`: \"read-only\" tools the assistant can use without user confirmation.\n",
    "4. `*_sensitive_tools`: tools with \"write\" access that require user confirmation (and will be assigned an `interrupt_before` when we compile the graph)\n",
    "5. `leave_skill`: _pop_ the `dialog_state` to signal that the *primary assistant* is back in control\n",
    "\n",
    "Because of their similarities, we _could_ define a factory function to generate these. Since this is a tutorial, we'll define them each explicitly.\n",
    "\n",
    "First, make the **flight booking assistant** dedicated to managing the user journey for updating and canceling flights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54297dc5-80b2-4bc6-8087-803caf1e0cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flight booking assistant\n",
    "builder.add_node(\n",
    "    \"enter_update_flight\",\n",
    "    create_entry_node(\"Flight Updates & Booking Assistant\", \"update_flight\"),\n",
    ")\n",
    "builder.add_node(\"update_flight\", Assistant(update_flight_runnable))\n",
    "builder.add_edge(\"enter_update_flight\", \"update_flight\")\n",
    "builder.add_node(\n",
    "    \"update_flight_sensitive_tools\",\n",
    "    create_tool_node_with_fallback(update_flight_sensitive_tools),\n",
    ")\n",
    "builder.add_node(\n",
    "    \"update_flight_safe_tools\",\n",
    "    create_tool_node_with_fallback(update_flight_safe_tools),\n",
    ")\n",
    "\n",
    "\n",
    "def route_update_flight(\n",
    "    state: State,\n",
    ") -> Literal[\n",
    "    \"update_flight_sensitive_tools\",\n",
    "    \"update_flight_safe_tools\",\n",
    "    \"leave_skill\",\n",
    "    \"__end__\",\n",
    "]:\n",
    "    route = tools_condition(state)\n",
    "    if route == END:\n",
    "        return END\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    did_cancel = any(tc[\"name\"] == CompleteOrEscalate.__name__ for tc in tool_calls)\n",
    "    if did_cancel:\n",
    "        return \"leave_skill\"\n",
    "    safe_toolnames = [t.name for t in update_flight_safe_tools]\n",
    "    if all(tc[\"name\"] in safe_toolnames for tc in tool_calls):\n",
    "        return \"update_flight_safe_tools\"\n",
    "    return \"update_flight_sensitive_tools\"\n",
    "\n",
    "\n",
    "builder.add_edge(\"update_flight_sensitive_tools\", \"update_flight\")\n",
    "builder.add_edge(\"update_flight_safe_tools\", \"update_flight\")\n",
    "builder.add_conditional_edges(\"update_flight\", route_update_flight)\n",
    "\n",
    "\n",
    "# This node will be shared for exiting all specialized assistants\n",
    "def pop_dialog_state(state: State) -> dict:\n",
    "    \"\"\"Pop the dialog stack and return to the main assistant.\n",
    "\n",
    "    This lets the full graph explicitly track the dialog flow and delegate control\n",
    "    to specific sub-graphs.\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "    if state[\"messages\"][-1].tool_calls:\n",
    "        # Note: Doesn't currently handle the edge case where the llm performs parallel tool calls\n",
    "        messages.append(\n",
    "            ToolMessage(\n",
    "                content=\"Resuming dialog with the host assistant. Please reflect on the past conversation and assist the user as needed.\",\n",
    "                tool_call_id=state[\"messages\"][-1].tool_calls[0][\"id\"],\n",
    "            )\n",
    "        )\n",
    "    return {\n",
    "        \"dialog_state\": \"pop\",\n",
    "        \"messages\": messages,\n",
    "    }\n",
    "\n",
    "\n",
    "builder.add_node(\"leave_skill\", pop_dialog_state)\n",
    "builder.add_edge(\"leave_skill\", \"primary_assistant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706e40ee-2f75-4a5a-bfbc-233b0e7b7eb4",
   "metadata": {},
   "source": [
    "Next, create the **car rental assistant** graph to own all car rental needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68b93f5-0f72-4e94-8e8b-b501ec82edcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Car rental assistant\n",
    "\n",
    "builder.add_node(\n",
    "    \"enter_book_car_rental\",\n",
    "    create_entry_node(\"Car Rental Assistant\", \"book_car_rental\"),\n",
    ")\n",
    "builder.add_node(\"book_car_rental\", Assistant(book_car_rental_runnable))\n",
    "builder.add_edge(\"enter_book_car_rental\", \"book_car_rental\")\n",
    "builder.add_node(\n",
    "    \"book_car_rental_safe_tools\",\n",
    "    create_tool_node_with_fallback(book_car_rental_safe_tools),\n",
    ")\n",
    "builder.add_node(\n",
    "    \"book_car_rental_sensitive_tools\",\n",
    "    create_tool_node_with_fallback(book_car_rental_sensitive_tools),\n",
    ")\n",
    "\n",
    "\n",
    "def route_book_car_rental(\n",
    "    state: State,\n",
    ") -> Literal[\n",
    "    \"book_car_rental_safe_tools\",\n",
    "    \"book_car_rental_sensitive_tools\",\n",
    "    \"leave_skill\",\n",
    "    \"__end__\",\n",
    "]:\n",
    "    route = tools_condition(state)\n",
    "    if route == END:\n",
    "        return END\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    did_cancel = any(tc[\"name\"] == CompleteOrEscalate.__name__ for tc in tool_calls)\n",
    "    if did_cancel:\n",
    "        return \"leave_skill\"\n",
    "    safe_toolnames = [t.name for t in book_car_rental_safe_tools]\n",
    "    if all(tc[\"name\"] in safe_toolnames for tc in tool_calls):\n",
    "        return \"book_car_rental_safe_tools\"\n",
    "    return \"book_car_rental_sensitive_tools\"\n",
    "\n",
    "\n",
    "builder.add_edge(\"book_car_rental_sensitive_tools\", \"book_car_rental\")\n",
    "builder.add_edge(\"book_car_rental_safe_tools\", \"book_car_rental\")\n",
    "builder.add_conditional_edges(\"book_car_rental\", route_book_car_rental)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e8aa17-8562-4fe8-9418-69703ec1946b",
   "metadata": {},
   "source": [
    "Then define the **hotel booking** workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec40edb9-d415-4f43-8f9f-c82a239c607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hotel booking assistant\n",
    "builder.add_node(\n",
    "    \"enter_book_hotel\", create_entry_node(\"Hotel Booking Assistant\", \"book_hotel\")\n",
    ")\n",
    "builder.add_node(\"book_hotel\", Assistant(book_hotel_runnable))\n",
    "builder.add_edge(\"enter_book_hotel\", \"book_hotel\")\n",
    "builder.add_node(\n",
    "    \"book_hotel_safe_tools\",\n",
    "    create_tool_node_with_fallback(book_hotel_safe_tools),\n",
    ")\n",
    "builder.add_node(\n",
    "    \"book_hotel_sensitive_tools\",\n",
    "    create_tool_node_with_fallback(book_hotel_sensitive_tools),\n",
    ")\n",
    "\n",
    "\n",
    "def route_book_hotel(\n",
    "    state: State,\n",
    ") -> Literal[\n",
    "    \"leave_skill\", \"book_hotel_safe_tools\", \"book_hotel_sensitive_tools\", \"__end__\"\n",
    "]:\n",
    "    route = tools_condition(state)\n",
    "    if route == END:\n",
    "        return END\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    did_cancel = any(tc[\"name\"] == CompleteOrEscalate.__name__ for tc in tool_calls)\n",
    "    if did_cancel:\n",
    "        return \"leave_skill\"\n",
    "    tool_names = [t.name for t in book_hotel_safe_tools]\n",
    "    if all(tc[\"name\"] in tool_names for tc in tool_calls):\n",
    "        return \"book_hotel_safe_tools\"\n",
    "    return \"book_hotel_sensitive_tools\"\n",
    "\n",
    "\n",
    "builder.add_edge(\"book_hotel_sensitive_tools\", \"book_hotel\")\n",
    "builder.add_edge(\"book_hotel_safe_tools\", \"book_hotel\")\n",
    "builder.add_conditional_edges(\"book_hotel\", route_book_hotel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c40aa1-b820-4d0a-9c08-76a8ad16044b",
   "metadata": {},
   "source": [
    "After that, define the **excursion assistant**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce9cf21-f708-4033-bca6-5f5d110b5662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excursion assistant\n",
    "builder.add_node(\n",
    "    \"enter_book_excursion\",\n",
    "    create_entry_node(\"Trip Recommendation Assistant\", \"book_excursion\"),\n",
    ")\n",
    "builder.add_node(\"book_excursion\", Assistant(book_excursion_runnable))\n",
    "builder.add_edge(\"enter_book_excursion\", \"book_excursion\")\n",
    "builder.add_node(\n",
    "    \"book_excursion_safe_tools\",\n",
    "    create_tool_node_with_fallback(book_excursion_safe_tools),\n",
    ")\n",
    "builder.add_node(\n",
    "    \"book_excursion_sensitive_tools\",\n",
    "    create_tool_node_with_fallback(book_excursion_sensitive_tools),\n",
    ")\n",
    "\n",
    "\n",
    "def route_book_excursion(\n",
    "    state: State,\n",
    ") -> Literal[\n",
    "    \"book_excursion_safe_tools\",\n",
    "    \"book_excursion_sensitive_tools\",\n",
    "    \"leave_skill\",\n",
    "    \"__end__\",\n",
    "]:\n",
    "    route = tools_condition(state)\n",
    "    if route == END:\n",
    "        return END\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    did_cancel = any(tc[\"name\"] == CompleteOrEscalate.__name__ for tc in tool_calls)\n",
    "    if did_cancel:\n",
    "        return \"leave_skill\"\n",
    "    tool_names = [t.name for t in book_excursion_safe_tools]\n",
    "    if all(tc[\"name\"] in tool_names for tc in tool_calls):\n",
    "        return \"book_excursion_safe_tools\"\n",
    "    return \"book_excursion_sensitive_tools\"\n",
    "\n",
    "\n",
    "builder.add_edge(\"book_excursion_sensitive_tools\", \"book_excursion\")\n",
    "builder.add_edge(\"book_excursion_safe_tools\", \"book_excursion\")\n",
    "builder.add_conditional_edges(\"book_excursion\", route_book_excursion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd73cdd-e50f-4819-82a4-d867359f9bb6",
   "metadata": {},
   "source": [
    "Finally, create the **primary assistant**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb19faf-66c8-4fd8-89ec-4d97d510ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary assistant\n",
    "builder.add_node(\"primary_assistant\", Assistant(assistant_runnable))\n",
    "builder.add_node(\n",
    "    \"primary_assistant_tools\", create_tool_node_with_fallback(primary_assistant_tools)\n",
    ")\n",
    "\n",
    "\n",
    "def route_primary_assistant(\n",
    "    state: State,\n",
    ") -> Literal[\n",
    "    \"primary_assistant_tools\",\n",
    "    \"enter_update_flight\",\n",
    "    \"enter_book_hotel\",\n",
    "    \"enter_book_excursion\",\n",
    "    \"__end__\",\n",
    "]:\n",
    "    route = tools_condition(state)\n",
    "    if route == END:\n",
    "        return END\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    if tool_calls:\n",
    "        if tool_calls[0][\"name\"] == ToFlightBookingAssistant.__name__:\n",
    "            return \"enter_update_flight\"\n",
    "        elif tool_calls[0][\"name\"] == ToBookCarRental.__name__:\n",
    "            return \"enter_book_car_rental\"\n",
    "        elif tool_calls[0][\"name\"] == ToHotelBookingAssistant.__name__:\n",
    "            return \"enter_book_hotel\"\n",
    "        elif tool_calls[0][\"name\"] == ToBookExcursion.__name__:\n",
    "            return \"enter_book_excursion\"\n",
    "        return \"primary_assistant_tools\"\n",
    "    raise ValueError(\"Invalid route\")\n",
    "\n",
    "\n",
    "# The assistant can route to one of the delegated assistants,\n",
    "# directly use a tool, or directly respond to the user\n",
    "builder.add_conditional_edges(\n",
    "    \"primary_assistant\",\n",
    "    route_primary_assistant,\n",
    "    {\n",
    "        \"enter_update_flight\": \"enter_update_flight\",\n",
    "        \"enter_book_car_rental\": \"enter_book_car_rental\",\n",
    "        \"enter_book_hotel\": \"enter_book_hotel\",\n",
    "        \"enter_book_excursion\": \"enter_book_excursion\",\n",
    "        \"primary_assistant_tools\": \"primary_assistant_tools\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "builder.add_edge(\"primary_assistant_tools\", \"primary_assistant\")\n",
    "\n",
    "\n",
    "# Each delegated workflow can directly respond to the user\n",
    "# When the user responds, we want to return to the currently active workflow\n",
    "def route_to_workflow(\n",
    "    state: State,\n",
    ") -> Literal[\n",
    "    \"primary_assistant\",\n",
    "    \"update_flight\",\n",
    "    \"book_car_rental\",\n",
    "    \"book_hotel\",\n",
    "    \"book_excursion\",\n",
    "]:\n",
    "    \"\"\"If we are in a delegated state, route directly to the appropriate assistant.\"\"\"\n",
    "    dialog_state = state.get(\"dialog_state\")\n",
    "    if not dialog_state:\n",
    "        return \"primary_assistant\"\n",
    "    return dialog_state[-1]\n",
    "\n",
    "\n",
    "builder.add_conditional_edges(\"fetch_user_info\", route_to_workflow)\n",
    "\n",
    "# Compile graph\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "part_4_graph = builder.compile(\n",
    "    checkpointer=memory,\n",
    "    # Let the user approve or deny the use of sensitive tools\n",
    "    interrupt_before=[\n",
    "        \"update_flight_sensitive_tools\",\n",
    "        \"book_car_rental_sensitive_tools\",\n",
    "        \"book_hotel_sensitive_tools\",\n",
    "        \"book_excursion_sensitive_tools\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a6f01e-4779-45e3-9e18-376cf05c6065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(part_4_graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3eb142-709d-4c29-9dbf-a34c5e800343",
   "metadata": {},
   "source": [
    "#### Conversation\n",
    "\n",
    "That was a lot! Let's run it over the following list of dialog turns. This time, we'll have many fewer confirmations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a548d-029a-47b7-9ac0-9c5203ec92c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import uuid\n",
    "\n",
    "# Update with the backup file so we can restart from the original place in each section\n",
    "shutil.copy(backup_file, db)\n",
    "thread_id = str(uuid.uuid4())\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # The passenger_id is used in our flight tools to\n",
    "        # fetch the user's flight information\n",
    "        \"passenger_id\": \"3442 587242\",\n",
    "        # Checkpoints are accessed by thread_id\n",
    "        \"thread_id\": thread_id,\n",
    "    }\n",
    "}\n",
    "\n",
    "_printed = set()\n",
    "# We can reuse the tutorial questions from part 1 to see how it does.\n",
    "for question in tutorial_questions:\n",
    "    events = part_4_graph.stream(\n",
    "        {\"messages\": (\"user\", question)}, config, stream_mode=\"values\"\n",
    "    )\n",
    "    for event in events:\n",
    "        _print_event(event, _printed)\n",
    "    snapshot = part_4_graph.get_state(config)\n",
    "    while snapshot.next:\n",
    "        # We have an interrupt! The agent is\n",
    "        # trying to use a tool.\n",
    "        # The user can approve or deny it\n",
    "        user_input = input(\n",
    "            \"Do you approve of the above actions? Type 'y' to continue;\"\n",
    "            \" otherwise, explain your requested changed.\\n\\n\"\n",
    "        )\n",
    "        if user_input.strip() == \"y\":\n",
    "            # Just continue\n",
    "            result = part_4_graph.invoke(\n",
    "                None,\n",
    "                config,\n",
    "            )\n",
    "        else:\n",
    "            # Satisfy the tool invocation by\n",
    "            # providing instructions on the requested changes / change of mind\n",
    "            result = part_4_graph.invoke(\n",
    "                {\n",
    "                    \"messages\": [\n",
    "                        ToolMessage(\n",
    "                            tool_call_id=event[\"messages\"][-1].tool_calls[0][\"id\"],\n",
    "                            content=f\"API call denied by user. Reasoning: '{user_input}'. Continue assisting, accounting for the user's input.\",\n",
    "                        )\n",
    "                    ]\n",
    "                },\n",
    "                config,\n",
    "            )\n",
    "        snapshot = part_4_graph.get_state(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764f2c09-d9ff-4f14-8507-5018c17edbb3",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "\n",
    "You've now developed a customer support bot that handles diverse tasks using focused workflows.\n",
    "More importantly, you've learned to use some of LangGraph's core features to design and refactor an application based on your product needs.\n",
    "\n",
    "The above examples are by no means optimized for your unique needs - LLMs make mistakes, and each flow can be made more reliable through better prompts and experimentation. Once you've created your initial support bot, the next step would be to start [adding evaluations](https://docs.smith.langchain.com/evaluation) so you can confidently improve your system. Check out those docs and our other tutorials to learn more!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
